<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.ML.OnnxRuntime</name>
    </assembly>
    <members>
        <member name="T:Microsoft.ML.OnnxRuntime.IDisposableReadOnlyCollection`1">
            <summary>
            Return immutable collection of results
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue">
            <summary>
            This is a legacy class that is kept for backward compatibility.
            Use OrtValue based API.
            
            This class serves as a container for model run output values including
            tensors, sequences of tensors, sequences and maps.
            The class must be disposed of.
            It disposes of _ortValueHolder that owns the underlying Ort output value and
            anything else that would need to be disposed by the instance of the class.
            Use factory method CreateFromOrtValue to obtain an instance of the class.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,Microsoft.ML.OnnxRuntime.IOrtValueOwner)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.OnnxValueType,Microsoft.ML.OnnxRuntime.IOrtValueOwner)">
            <summary>
            Ctor for non-tensor values
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <param name="onnxValueType"></param>
            <param name="ortValueHolder"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.MapHelper,Microsoft.ML.OnnxRuntime.IOrtValueOwner)">
            <summary>
            Construct an instance that would contain a map in a form of a Dictionary
            Currently a limited number of primitive types are supported as map keys and values.
            So this is not a full implementation of the map type.
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <param name="mapHelper"></param>
            <param name="ortValueHolder"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.ElementType">
            <summary>
            Only valid if ValueType is Tensor
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.InputToOrtValueHandle(Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Overrides the base class method. With respect to pinnedMemoryHandle, it has no operation
            to do, as this class maintains a native buffer via _ortValueHolder and the memory will be
            disposed by it. This is the case when we are dealing with an OrtValue that is backed by native memory
            and not by pinned managed memory.
            
            This class is generally used for outputs to be created on top of the output OrtValue,
            but the interface (derived from NamedOnnxValue) allows it to be passed as output and one of the test
            cases does it. Unless we deprecate and re-do the interface, we must support it.
            </summary>
            <param name="pinnedMemoryHandle">always set to null</param>
            <returns>Native OrtValue handle</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.OutputToOrtValueHandle(Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Generally, this class is created on top of the values that are returned by the model run.
            However, there is a test case that uses this value for output
            It will return the OrtValue that was previously created, since the caller must understand what they are doing.
            </summary>
            <param name="metadata"></param>
            <param name="memoryOwner"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.CreateFromOrtValue(System.String,Microsoft.ML.OnnxRuntime.OrtValue@)">
            <summary>
            This function takes ortValue and constructs an instance of DisposableNamedOnnxValue.
            The new instance takes ownership of the OrtValue and will dispose of it when it is disposed of.
            </summary>
            <param name="name"></param>
            <param name="ortValue">becomes null on success.</param>
            <returns>an instance of DisposableNamedOnnxValue</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.CreateFromOrtValue(System.String,Microsoft.ML.OnnxRuntime.OrtValue@,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            This function takes ortValue and constructs an instance of DisposableNamedOnnxValue.
            The new instance takes ownership of the OrtValue and will dispose of it when it is disposed of.
            </summary>
            <param name="name"></param>
            <param name="ortValue">becomes null on success.</param>
            <param name="allocator"></param>
            <returns>an instance of DisposableNamedOnnxValue</returns>
            <exception cref="T:System.NotSupportedException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeTensor(System.String,Microsoft.ML.OnnxRuntime.OrtValue@)">
            <summary>
            Creates an instance of DisposableNamedOnnxValue and takes ownership of ortValue.
            on success.
            </summary>
            <param name="name">name of the value</param>
            <param name="ortValue">Underlying OrtValue. This becomes null on successful return.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeTensor``1(System.String,Microsoft.ML.OnnxRuntime.OrtValue@)">
            <summary>
            This method creates an instance of DisposableNamedOnnxValue that has possession of ortValueElement
            native memory Tensor and returns it to the caller.
            </summary>
            <typeparam name="T">data type</typeparam>
            <param name="name">name of the output</param>
            <param name="ortValue">native tensor. Becomes null on successful return.</param>
            <returns>DisposableNamedOnnxValue instance</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeSequence(System.String,Microsoft.ML.OnnxRuntime.OrtValue@,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            This method will create an instance of DisposableNamedOnnxValue that will own ortSequenceValue
            an all disposable native objects that are elements of the sequence
            </summary>
            <param name="name"></param>
            <param name="ortValueSequence">ortValueElement that has native sequence</param>
            <param name="allocator"> used allocator</param>
            <returns>DisposableNamedOnnxValue</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeMap(System.String,Microsoft.ML.OnnxRuntime.OrtValue@,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            Will extract keys and values from the map and create a DisposableNamedOnnxValue from it
            </summary>
            <param name="name">name of the output</param>
            <param name="ortValueMap">ortValue that represents a map. Becomes null on success</param>
            <param name="allocator"></param>
            <returns>DisposableNamedOnnxValue</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeMapElements``2(System.String,Microsoft.ML.OnnxRuntime.OrtValue@,System.Int32[],Microsoft.ML.OnnxRuntime.OrtValue@,System.Int32[],Microsoft.ML.OnnxRuntime.OrtValue@)">
            <summary>
            This method maps keys and values of the map and copies them into a managed Dictionary
            and returns as an instance of DisposableNamedOnnxValue. The method takes possession of ortValueMap,
            ortValueTensorKeys and ortValueTensorValues and disposes of them.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="name"></param>
            <param name="ortValueMap">becomes null on success return</param>
            <param name="keysShape">keys shape in ints</param>
            <param name="ortValueTensorKeys">becomes null on success</param>
            <param name="valsShape">values shape in ints</param>
            <param name="ortValueTensorValues">becomes null on success</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked by Dispose()</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ErrorCode">
            <summary>
            Enum conresponding to native onnxruntime error codes. Must be in sync with the native API
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException">
            <summary>
            The Exception that is thrown for errors related ton OnnxRuntime
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue">
            <summary>
            This is a legacy class that is kept for backward compatibility.
            Use OrtValue based API.
            
            Represents an OrtValue with its underlying buffer pinned
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue.CreateFromTensor``1(Microsoft.ML.OnnxRuntime.Tensors.Tensor{``0})">
            <summary>
            Creates a <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> object from the tensor and pins its underlying buffer.
            </summary>
            <typeparam name="T"></typeparam>
            <param name="value"></param>
            <returns>a disposable instance of FixedBufferOnnxValue</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue.CreateFromMemory``1(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,System.Memory{``0},Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],System.Int64)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked from Dispose()</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.InferenceSession">
            <summary>
            Represents an Inference Session on an ONNX Model.
            This is a IDisposable class and it must be disposed of
            using either a explicit call to Dispose() method or
            a pattern of using() block. If this is a member of another
            class that class must also become IDisposable and it must
            dispose of InferenceSession in its Dispose() method.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._nativeHandle">
            <summary>
            A pointer to a underlying native instance of OrtSession
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._inputMetadata">
            <summary>
            Dictionary that represents input metadata
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._inputNames">
            <summary>
            Ordered list of input names
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._outputMetadata">
            <summary>
            Dictionary that represent output metadata
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._outputNames">
            <summary>
            Ordered list of output names
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._overridableInitializerMetadata">
            <summary>
            Dictionary that represents overridableInitializers metadata
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._namesMemoryPtrs">
            <summary>
            This list holds Utf-8 converted input/output names allocated from a native heap
            and as such do not require pinning. It must be disposed of (freed).
            
            Introduced to reduce the GC burden as the names are used in every Run() call.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.String)">
            <summary>
            Constructs an InferenceSession from a model file
            </summary>
            <param name="modelPath"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.String,Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer)">
            <summary>
            Constructs an InferenceSession from a model file and it will use 
            the provided pre-packed weights container to store and share pre-packed buffers 
            of shared initializers across sessions if any.
            </summary>
            <param name="modelPath">Model path</param>
            <param name="prepackedWeightsContainer">Instance of PrepackedWeightsContainer. 
            Lifetime of 'prepackedWeightsContainer' must be
            managed by the user and it must outlive any sessions reliant on it</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.String,Microsoft.ML.OnnxRuntime.SessionOptions)">
            <summary>
            Constructs an InferenceSession from a model file, with some additional session options
            </summary>
            <param name="modelPath"></param>
            <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.String,Microsoft.ML.OnnxRuntime.SessionOptions,Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer)">
            <summary>
            Constructs an InferenceSession from a model file, with some additional session options
            and it will use the provided pre-packed weights container to store and share pre-packed buffers 
            of shared initializers across sessions if any.
            </summary>
            <param name="modelPath">Model path</param>
            <param name="options">Session options</param>
            <param name="prepackedWeightsContainer">Instance of PrepackedWeightsContainer. 
            Lifetime of 'prepackedWeightsContainer' must be
            managed by the user and it must outlive any sessions reliant on it</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.Byte[])">
            <summary>
            Constructs an InferenceSession from a model data in byte array
            </summary>
            <param name="model"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.Byte[],Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer)">
            <summary>
            Constructs an InferenceSession from a model data (in byte array) and it will use 
            the provided pre-packed weights container to store and share pre-packed buffers 
            of shared initializers across sessions if any.
            </summary>
            <param name="model">Model as byte array</param>
            <param name="prepackedWeightsContainer">Instance of PrepackedWeightsContainer. 
            Lifetime of 'prepackedWeightsContainer' must be
            managed by the user and it must outlive any sessions reliant on it</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.Byte[],Microsoft.ML.OnnxRuntime.SessionOptions)">
            <summary>
            Constructs an InferenceSession from a model data in byte array, with some additional session options
            </summary>
            <param name="model"></param>
            <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.Byte[],Microsoft.ML.OnnxRuntime.SessionOptions,Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer)">
            <summary>
            Constructs an InferenceSession from a model data (in byte array) with some additional
            session options and it will use the provided pre-packed weights container to store
            and share pre-packed buffers of shared initializers across sessions if any.
            </summary>
            <param name="model">Model as byte array</param>
            <param name="options">Session Options</param>
            <param name="prepackedWeightsContainer">Instance of PrepackedWeightsContainer. 
            Lifetime of 'prepackedWeightsContainer' must be
            managed by the user and it must outlive any sessions reliant on it</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.InputMetadata">
            <summary>
            Meta data regarding the input nodes, keyed by input names
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.InputNames">
            <summary>
            Ordered list of input names that can be accessed by index;
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.OutputMetadata">
            <summary>
            Metadata regarding the output nodes, keyed by output names
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.OutputNames">
            <summary>
            Ordered list of output names that can be accessed by index.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.OverridableInitializerMetadata">
            <summary>
            Metadata regarding the overridable initializers, keyed by node names
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs, and fetches all the outputs.
            </summary>
            <param name="inputs">specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            Runs the loaded model for the given inputs, and fetches the outputs specified in <paramref name="outputNames"/>.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names to fetch.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs the loaded model for the given inputs, and fetches the specified outputs in <paramref name="outputNames"/>. Uses the given RunOptions for this run.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names to fetch.</param>
            <param name="options"></param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs, and fetches all the outputs.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            Runs the loaded model for the given inputs, and fetches the outputs specified in <paramref name="outputNames"/>.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names to fetch.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs the loaded model for the given inputs, and fetches the specified outputs in <paramref name="outputNames"/>. Uses the given RunOptions for this run.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names to fetch.</param>
            <param name="options"></param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs and outputs.
            
            Outputs need to be created with correct type and dimension to accept the fetched data.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names. Should match <paramref name="outputValues"/>.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs the loaded model for the given inputs and outputs. Uses the given RunOptions for this run.
            
            Outputs need to be created with correct type and dimension to accept the fetched data.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names. Should match <paramref name="outputValues"/>.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
            <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs and outputs.
            
            Outputs need to be created with correct type and dimension to receive the fetched data.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},Microsoft.ML.OnnxRuntime.RunOptions)">
             <summary>
             
             Runs the loaded model for the given inputs and outputs. Uses the given RunOptions for this run.
            
             Outputs need to be created with correct type and dimension to receive the fetched data.
             </summary>
             <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
             <param name="outputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the output values.</param>
             <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs and outputs.
            
            Outputs need to be created with correct type and dimension to receive the fetched data.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names. Should match <paramref name="outputValues"/>.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs the loaded model for the given inputs and outputs. Uses the given RunOptions for this run.
            
            Outputs need to be created with correct type and dimension to receive the fetched data.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names. Should match <paramref name="outputValues"/>.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
            <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue})">
             <summary>
            
             Runs the loaded model for the given inputs and outputs.
            
             Outputs need to be created with correct type and dimension to receive the fetched data.
             </summary>
             <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
             <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
             <param name="outputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},Microsoft.ML.OnnxRuntime.RunOptions)">
             <summary>
             
             Runs the loaded model for the given inputs and outputs. Uses the given RunOptions for this run.
            
             Outputs need to be created with correct type and dimension to receive the fetched data.
             </summary>
             <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
             <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
             <param name="outputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the output values.</param>
             <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.OrtValue},System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            The API runs the inference taking a collection of OrtValues as input and
            returning a collection of output OrtValues.
            </summary>
            <param name="runOptions">runOptions</param>
            <param name="inputNames">A collection of input names.
            To supply all names, use InputNames property</param>
            <param name="inputValues">Input OrtValues. The size of the collection must match the size and the order of the inputNames</param>
            <param name="outputNames">Output names requested. To supply all names, use OutputNames property.</param>
            <returns>A disposable collection of disposable OrtValues</returns>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyDictionary{System.String,Microsoft.ML.OnnxRuntime.OrtValue},System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            This API takes inputs as a dictionary of input names paired with input OrtValues
            
            It returns a disposable collection of OrtValues for outputs that were designated by outputNames
            </summary>
            <param name="runOptions"></param>
            <param name="inputs">Dictionary of name/value pairs</param>
            <param name="outputNames">requested outputs. To request all outputs, use OutputNames property of this sessions</param>
            <returns>A disposable collection of outputs</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.OrtValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.OrtValue})">
            <summary>
            The API takes collections of inputNames/inputValues and collections of outputNames/outputValues.
            The sizes of the corresponding collections must match.
            
            The output OrtValues are pre-allocated and the API will fill the data into the OrtValues.
            These MUST be tensors. The API does not support non-tensor types for output values.
            
            The API is useful when the output values are tensors and their shapes are known, and you
            prefer the output to go to the pre-allocated memory. In such a case, you create
            output OrtValues over those pre-allocated buffers and pass them to the API.
            </summary>
            <param name="runOptions">runOptions, if null the defaults are used</param>
            <param name="inputNames">collection of input names.</param>
            <param name="inputValues">collection of input OrtValues. Must match the order and the number of input names.</param>
            <param name="outputNames">Requested output names.</param>
            <param name="outputValues">Pre-allocated output values. 
            The order and the number must match the specified output names. Shapes must match actual output values.</param>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.CreateIoBinding">
            <summary>
            Create OrtIoBinding instance to bind pre-allocated buffers
            to input/output
            </summary>
            <returns>A new instance of OrtIoBinding</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.RunWithBinding(Microsoft.ML.OnnxRuntime.RunOptions,Microsoft.ML.OnnxRuntime.OrtIoBinding)">
            <summary>
            This method runs inference on the OrtIoBinding instance
            The method does not return anything. This is a lightweight version of 
            RunWithBindingAndNames(). When you bind pre-allocated buffers to the output values
            you may not want to fetch the outputs since you already have access to them so you can spare
            the expense of fetching them and pairing with names.
            You can still fetch the outputs by calling OrtIOBinding.GetOutputValues()
            </summary>
            <param name="runOptions">runOptions</param>
            <param name="ioBinding">ioBinding instance to use</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.RunWithBoundResults(Microsoft.ML.OnnxRuntime.RunOptions,Microsoft.ML.OnnxRuntime.OrtIoBinding)">
            <summary>
            This method runs inference on the OrtIoBinding instance. It returns a collection of OrtValues.
            This method is useful when it is impossible to bind outputs to pre-allocated buffers, because
            the output shape is not known in advance. In this case, the OrtValues returned by this method
            are allocated and owned by ORT. The caller is responsible for disposing the collection.
            </summary>
            <param name="runOptions">RunOptions</param>
            <param name="ioBinding">Binding instance</param>
            <returns>A disposable collection of OrtValues</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.RunWithBindingAndNames(Microsoft.ML.OnnxRuntime.RunOptions,Microsoft.ML.OnnxRuntime.OrtIoBinding,System.String[])">
            <summary>
             This method return a collection of DisposableNamedOnnxValue as in other interfaces
             Query names from OrtIoBinding object and pair then with the array of OrtValues returned
            from OrtIoBinding.GetOutputValues().
            
            This API will be deprecated in favor of the API that returns a collection of OrtValues.
            
            </summary>
            <param name="runOptions">RunOptions</param>
            <param name="ioBinding">OrtIoBinding instance with bindings</param>
            <param name="names">optional parameter. If you already know the names of the outputs you can save a native
            call to retrieve output names. They will be paired with the returned OrtValues and combined into DisposbleNamedOnnxValues.
            Otherwise, the method will retrieve output names from the OrtIoBinding instance.
            It is an error if you supply a different number of names than the returned outputs</param>
            <returns>A disposable collection of DisposableNamedOnnxValue that encapsulate output OrtValues</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.EndProfiling">
            <summary>
            Ends profiling for the session.
            </summary>
            <returns> Returns the profile file name.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.LookupInputMetadata(System.String)">
            <summary>
            Checks if the name is a known input or overridable initializer name
            and if so, returns metadata for it.
            metadata
            </summary>
            <param name="nodeName"></param>
            <returns>NodeMetadata for the nodeName</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.LookupOutputMetadata(System.String)">
            <summary>
            Checks if the nodeName is a known output name and if so returns metadata for it.
            </summary>
            <param name="nodeName"></param>
            <returns></returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.ExtractOrtValueHandleForInput(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Fetches/creates OrtValue for the content of the input
            </summary>
            <param name="input"></param>
            <param name="metadata"></param>
            <param name="memOwner"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.ExtractOrtValueHandleForOutput(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Fetches/Creates OrtValue for output
            </summary>
            <param name="output"></param>
            <param name="metadata"></param>
            <param name="memOwner"></param>
            <returns>May return null if the onnx value type does not support pre-creation of output OrtValues</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.LookupUtf8Names``1(System.Collections.Generic.IReadOnlyCollection{``0},Microsoft.ML.OnnxRuntime.InferenceSession.NameExtractor{``0},Microsoft.ML.OnnxRuntime.InferenceSession.MetadataLookup)">
            <summary>
            Run helper
            </summary>
            <param name="values">names to convert to zero terminated utf8 and pin</param>
            <param name="nameExtractor">extractor functor that helps extracting names from inputs</param>
            <param name="metaDict">inputs/outputs metadata</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.GetOrtValuesHandles(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},Microsoft.ML.OnnxRuntime.InferenceSession.MetadataLookup,Microsoft.ML.OnnxRuntime.InferenceSession.OrtValueHandleExtractor,Microsoft.ML.OnnxRuntime.DisposableArray{System.IDisposable}@)">
            <summary>
            This function obtains ortValues for NamedOnnxValue.
            The problem with NamedOnnxValue is that it is not disposable and can not contain any disposable items.
            so calling InputToOrtValue creates a new instance of OrtValue that needs to be disposed.
            The deriving object DisposableNamedValue actually contains and owns OrtValue and it returns
            it.
            </summary>
            <param name="values">a collection of NamedOnnxValues</param>
            <param name="metaLookup">Metadata lookup function (input/initializers/output)</param>
            <returns></returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.ModelMetadata">
            <summary>
            This property queries model metadata, constructs
            an instance of ModelMetadata and caches it
            </summary>
            <returns>Instance of ModelMetdata</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.ProfilingStartTimeNs">
            <summary>
            Return the nanoseconds of profiling's start time
            On some platforms, this timer may not be as precise as nanoseconds
            For instance, on Windows and MacOS, the precision will be ~100ns
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.RunAsync(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.OrtValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.OrtValue})">
            <summary>
            Run inference asynchronous in a thread of intra-op thread pool
            </summary>
            <param name="options">run option, can be null</param>
            <param name="inputNames">name of inputs</param>
            <param name="inputValues">input ort values</param>
            <param name="outputNames">name of outputs</param>
            <param name="outputValues">output of ort values</param>
            <returns>task to be awaited</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.InitWithSessionHandle(System.IntPtr)">
            <summary>
            Initializes the session object with a native session handle
            </summary>
            <param name="session">Value of a native session object</param>
            <param name="options">Session options</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.Handle">
            <summary>
            Other classes access
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Finalize">
            <summary>
            Finalizer. to cleanup session in case it runs
            and the user forgets to Dispose() of the session
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked from Dispose() method</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.DisposeImpl(System.Boolean)">
            <summary>
            This function is also used on failure in the constructor
            </summary>
            <param name="disposing"></param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.TensorTypeAndShape">
            <summary>
            Represents tensor element type and its shapes
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TensorTypeAndShape.ElementDataType">
            <summary>
            Tensor Element type
            </summary>
            <value>TensorElementType enum</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TensorTypeAndShape.Dimensions">
            <summary>
            Shape
            </summary>
            <value>Array of dimensions</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TensorTypeAndShape.SymbolicDimensions">
            <summary>
            Symbolic dimensions
            </summary>
            <value>Array of symbolic dimensions if present.</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TensorTypeAndShape.ElementTypeInfo">
            <summary>
            Tensor element metadata
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.SequenceMetadata">
            <summary>
            Represents sequnce metdata
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SequenceMetadata.#ctor(Microsoft.ML.OnnxRuntime.NodeMetadata)">
            <summary>
            __ctor
            </summary>
            <param name="elementData"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SequenceMetadata.ElementMeta">
            <summary>
            Element Metatada, recursive definition with a Tensor being a base case
            may contain maps, tensors and other sequences
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OptionalMetadata">
            <summary>
            The class contains metadata for an optional input/output
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OptionalMetadata.#ctor(Microsoft.ML.OnnxRuntime.NodeMetadata)">
            <summary>
            __ctor
            </summary>
            <param name="elementData"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OptionalMetadata.ElementMeta">
            <summary>
            Element Metatada, recursive definition with a Tensor being a base case
            may contain maps, tensors and sequences
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.MapMetadata">
            <summary>
            Represents Map MetaData.
            Key is always a tensor denoted by an element type
            with value type being a recursive structure that may
            contain other maps, sequences or tensors.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.MapMetadata.KeyDataType">
            <summary>
            Key tensor data type
            </summary>
            <value>A value of TensorElementType enum</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.MapMetadata.ValueMetadata">
            <summary>
            Value metadata
            </summary>
            /// <value>Instance of Nodemetadata for the value of the map</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NodeMetadata">
            <summary>
            Resembles type and shape information of session-graph nodes, used for communicating the shape/type of input/output nodes
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.#ctor(Microsoft.ML.OnnxRuntime.OnnxValueType,Microsoft.ML.OnnxRuntime.TensorTypeAndShape)">
            <summary>
            Constructs NodeMetadata for tensor
            </summary>
            <param name="onnxValueType">either ONNX_TYPE_TENSOR or ONNX_TYPE_SPARSETENSOR</param>
            <param name="typeAndShape">Tensor type and shape information</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.#ctor(Microsoft.ML.OnnxRuntime.MapMetadata)">
            <summary>
            __ctor for map metadata
            </summary>
            <param name="mapMetadata"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.#ctor(Microsoft.ML.OnnxRuntime.SequenceMetadata)">
            <summary>
            __ctor for sequence metadata
            </summary>
            <param name="sequenceMetadata"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.#ctor(Microsoft.ML.OnnxRuntime.OptionalMetadata)">
            <summary>
            __ctor
            </summary>
            <param name="optMetadata"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.AsMapMetadata">
            <summary>
            Retrieves MapMetadata, valid only if this node represents a Map.
            </summary>
            <returns></returns>
            <exception cref="T:System.InvalidOperationException">when the instance does not contain map metadata</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.AsSequenceMetadata">
            <summary>
            Retrieves SequenceMetadata, valid only if this node represents a Sequence
            </summary>
            <returns></returns>
            <exception cref="T:System.InvalidOperationException">when the instance does not contain sequence metadata</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.AsOptionalMetadata">
            <summary>
            Retrieves Optional type metadata, valid if this node is optional
            Optional metadata is nothing more than just a container for all the usual
            element types.
            </summary>
            <returns></returns>
            <exception cref="T:System.InvalidOperationException"></exception>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.OnnxValueType">
            <summary>
            Type value of the node
            </summary>
            <value>A value of OnnxValueType enum</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.ZeroTerminatedName">
            <summary>
            Node name in the natively allocated memory.
            
            Present only on the top-level instance
            metadata dictionary entries.
            
            Avoids repeated conversion and pinning
            
            This memory chunk is owned and freed by the InferenceSession
            object.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.Dimensions">
            <summary>
            Tensor shape valid only if this is a Tensor.
            Preserved for API compatibility
            </summary>
            <value>Array of dimensions</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.SymbolicDimensions">
            <summary>
            Symbolic dimensions valid only if this is a Tensor.
            Preserved for API compatibility
            </summary>
            <value>Array of symbolic dimensions if present.</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.ElementType">
            <summary>
            .NET type that corresponds to the primitive Tensor data type.
            Valid only if this is a Tensor.
            </summary>
            <value>System.Type</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.ElementDataType">
            <summary>
            Tensor Element Type. Valid if tensor
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.IsString">
            <summary>
            Convinience method to check for string
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.IsTensor">
            <summary>
            Whether it is a Tensor
            </summary>
            <value>currently always returns true</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ModelMetadata">
            <summary>
            A class that queries and caches model metadata and exposes
            it as properties
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.ProducerName">
            <summary>
            Producer name string
            </summary>
            <value>producer name string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.GraphName">
            <summary>
            Graph name for this model
            </summary>
            <value>graph name string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.Domain">
            <summary>
            Domain for this model
            </summary>
            <value>domain name string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.Description">
            <summary>
            Unstructured model description
            </summary>
            <value>description string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.GraphDescription">
            <summary>
            Unstructured graph description
            </summary>
            <value>description string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.Version">
            <summary>
            Version number
            </summary>
            <value>long version integer</value>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Microsoft.ML.OnnxRuntime.ModelMetadata.CustomMetadataMap" -->
        <member name="T:Microsoft.ML.OnnxRuntime.ManagedTypeProjection">
            <summary>
            The class helps to feed the NamedOnnxValue as inference input.
            It projects managed classes to OrtValues so they can be consumed
            by the native onnxruntime library. if possible, it will avoid copying data.
            The NamedOnnxValue can be a tensor, sequence or map.
            For recursive structures, create nested NamedOnnxValue instances.
            For example, a sequence instance would contain a list of NamedOnnxValue instances
            that in turn may represent tensors or other ONNX values.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.CreateProjection(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata)">
            <summary>
            Dispatches the creation of the projection
            </summary>
            <param name="namedOnnxValue"></param>
            <param name="metadata"></param>
            <returns>OrtValye created accoding to the metadata</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.CreateSequenceProjection(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.CreateMapProjection(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata)">
            <summary>
            Creates map projection. Since we support only primitive types in maps
            we map two tensors (keys and values)
            </summary>
            <param name="node"></param>
            <param name="elementMeta"></param>
            <returns>OrtValue</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.CreateTensorProjection(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata)">
            <summary>
            This pins memory that is contained within DenseTensor.
            </summary>
            <param name="node">NodeOnnxValue containing DenseTensor</param>
            <param name="elementMeta"></param>
            <returns></returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.MapHelper">
            <summary>
            The class holds keys and values for the dictionary
            in a for of two DenseTensors. The class is used to avoid
            data copy and make these available to the native code.
            Strings require special handling.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.NamedOnnxValue" -->
        <member name="F:Microsoft.ML.OnnxRuntime.NamedOnnxValue._value">
            <summary>
            Managed Tensor, Dictionary or IList
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.NamedOnnxValue._name">
            <summary>
            Name of the instance, model input/output
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.#ctor(System.String,System.Object)">
            <summary>
            Constructs an instance of NamedOnnxValue and represents
            a model input to an inference session.
            </summary>
            <param name="name">input/output name</param>
            <param name="value">Object that may be a tensor, Dictionary, IList</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.OnnxValueType)">
            <summary>
            Constructs an instance that contains a tensor, sequence or optional type.
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <param name="valueType"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.MapHelper)">
            <summary>
            Use this to construct maps
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <param name="helper"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NamedOnnxValue.ValueType">
            <summary>
            Onnx Value Type if known. In general, NamedOnnxValue is able to contain
            arbitrary objects. Please, follow the convention described in the class doc.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.CreateFromTensor``1(System.String,Microsoft.ML.OnnxRuntime.Tensors.Tensor{``0})">
            <summary>
            This is a factory method that instantiates NamedOnnxValue
            and associated name with an instance of a Tensor<typeparamref name="T"/>
            </summary>
            <typeparam name="T"></typeparam>
            <param name="name">name</param>
            <param name="value">Tensor<typeparamref name="T"/></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.CreateFromSequence``1(System.String,System.Collections.Generic.IEnumerable{``0})">
            <summary>
            This is a factory method that instantiates NamedOnnxValue.
            It would contain a sequence of elements
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.CreateFromMap``2(System.String,System.Collections.Generic.IDictionary{``0,``1})" -->
        <member name="P:Microsoft.ML.OnnxRuntime.NamedOnnxValue.Name">
            <summary>
            Exposes the name of the of the model input/output
            </summary>
            <value>name string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NamedOnnxValue.Value">
            <summary>
            Exposes the underlying managed object
            </summary>
            <value>object</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.AsTensor``1">
            <summary>
            Try-get value as a Tensor&lt;T&gt;.
            </summary>
            <typeparam name="T">Type</typeparam>
            <returns>Tensor object if contained value is a Tensor. Null otherwise</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.AsEnumerable``1">
            <summary>
            Try-get value as an Enumerable&lt;T&gt;.
            T is usually a NamedOnnxValue instance that may contain
            Tensors, Sequences, Maps or optional types
            </summary>
            <typeparam name="T">Type</typeparam>
            <returns>Enumerable object if contained value is a Enumerable. Null otherwise</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.AsDictionary``2">
            <summary>
            Try-get value as an Dictionary&lt;K,V&gt;.
            </summary>
            <typeparam name="K">Key type currently primitive type only</typeparam>
            <typeparam name="V">Value type, currently primitive type only</typeparam>
            <returns>Dictionary object if contained value is a Dictionary. Null otherwise</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.InputToOrtValueHandle(Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Pin the underlying memory and create an instance of OrtValue containing a tensor
            based on the pinned managed memory. The caller is responsible for Disposing
            both OrtValue and pinnedMemoryHandle
            </summary>
            <param name="pinnedMemoryHandle">dispose after returned OrtValus is disposed</param>
            <returns>The native OrtValue handle</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.OutputToOrtValueHandle(Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Produces an output value for outputs. This produces an output value
            only for tensors or optional types that can contain a tensor.
            For all other Onnx value types, this method throws. Use Run() overloads
            that return DisposableNamedOnnxValue to get access to all Onnx value types
            that may be returned as output.
            </summary>
            <param name="metadata"></param>
            <param name="memoryOwner"></param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.GetDictionaryKeys" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.GetDictionaryValues" -->
        <member name="M:Microsoft.ML.OnnxRuntime.NativeApiStatus.VerifySuccess(System.IntPtr)">
            <summary>
            Checks the native Status if the errocode is OK/Success. Otherwise constructs an appropriate exception and throws.
            Releases the native status object, as needed.
            </summary>
            <param name="nativeStatus"></param>
            <throws></throws>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateTensorRTProviderOptions">
            <summary>
            Creates native OrtTensorRTProviderOptions instance
            </summary>
            <param name="trtProviderOptionsInstance">(output) native instance of OrtTensorRTProviderOptions</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtUpdateTensorRTProviderOptions">
            <summary>
            Updates native OrtTensorRTProviderOptions instance using given key/value pairs
            </summary>
            <param name="trtProviderOptionsInstance">native instance of OrtTensorRTProviderOptions</param>
            <param name="providerOptionsKeys">configuration keys of OrtTensorRTProviderOptions</param>
            <param name="providerOptionsValues">configuration values of OrtTensorRTProviderOptions</param>
            <param name="numKeys">number of configuration keys</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetTensorRTProviderOptionsAsString">
            <summary>
            Get native OrtTensorRTProviderOptionsV2 in serialized string
            </summary>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="ptr">is a UTF-8 null terminated string allocated using 'allocator'</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseTensorRTProviderOptions">
            <summary>
            Releases native OrtTensorRTProviderOptions instance
            </summary>
            <param name="trtProviderOptionsInstance">native instance of OrtTensorRTProviderOptions to be released</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateCUDAProviderOptions">
            <summary>
            Creates native OrtCUDAProviderOptions instance
            </summary>
            <param name="cudaProviderOptionsInstance">(output) native instance of OrtCUDAProviderOptions</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtUpdateCUDAProviderOptions">
            <summary>
            Updates native OrtCUDAProviderOptions instance using given key/value pairs
            </summary>
            <param name="cudaProviderOptionsInstance">native instance of OrtCUDAProviderOptions</param>
            <param name="providerOptionsKeys">configuration keys of OrtCUDAProviderOptions</param>
            <param name="providerOptionsValues">configuration values of OrtCUDAProviderOptions</param>
            <param name="numKeys">number of configuration keys</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetCUDAProviderOptionsAsString">
            <summary>
            Get native OrtCUDAProviderOptionsV2 in serialized string
            </summary>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="ptr">is a UTF-8 null terminated string allocated using 'allocator'</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseCUDAProviderOptions">
            <summary>
            Releases native OrtCUDAProviderOptions instance
            </summary>
            <param name="cudaProviderOptionsInstance">native instance of OrtCUDAProviderOptions to be released</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateROCMProviderOptions">
            <summary>
            Creates native OrtROCMProviderOptions instance
            </summary>
            <param name="rocmProviderOptionsInstance">(output) native instance of OrtROCMProviderOptions</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtUpdateROCMProviderOptions">
            <summary>
            Updates native OrtROCMProviderOptions instance using given key/value pairs
            </summary>
            <param name="rocmProviderOptionsInstance">native instance of OrtROCMProviderOptions</param>
            <param name="providerOptionsKeys">configuration keys of OrtROCMProviderOptions</param>
            <param name="providerOptionsValues">configuration values of OrtROCMProviderOptions</param>
            <param name="numKeys">number of configuration keys</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetROCMProviderOptionsAsString">
            <summary>
            Get native OrtROCMProviderOptions in serialized string
            </summary>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="ptr">is a UTF-8 null terminated string allocated using 'allocator'</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseROCMProviderOptions">
            <summary>
            Releases native OrtROCMProviderOptions instance
            </summary>
            <param name="rocmProviderOptionsInstance">native instance of OrtROCMProviderOptions to be released</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateSessionWithPrepackedWeightsContainer">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="environment">Native OrtEnv instance</param>
            <param name="modelPath">UTF-8 bytes corresponding to model string path</param>
            <param name="sessionOptions">Native SessionOptions instance</param>
            <param name="prepackedWeightsContainer">Native OrtPrepackedWeightsContainer instance</param>
            <param name="session">(Output) Created native OrtSession instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateSessionFromArrayWithPrepackedWeightsContainer">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="environment">Native OrtEnv instance</param>
            <param name="modelData">Byte array correspoonding to the model</param>
            <param name="modelSize">Size of the model in bytes</param>
            <param name="sessionOptions">Native SessionOptions instance</param>
            <param name="prepackedWeightsContainer">Native OrtPrepackedWeightsContainer instance</param>
            <param name="session">(Output) Created native OrtSession instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddSessionConfigEntry">
            <summary>
            Add session config entry
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="configKey">Config key</param>
            <param name="configValue">Config value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeMethods.OrtSessionOptionsAppendExecutionProvider_CPU(System.IntPtr,System.Int32)">
            **
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_TensorRT">
            <summary>
            Append a TensorRT EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="trtProviderOptions">Native OrtTensorRTProviderOptions instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_TensorRT_V2">
            <summary>
            Append a TensorRT EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="trtProviderOptions">Native OrtTensorRTProviderOptionsV2 instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_CUDA">
            <summary>
            Append a CUDA EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="cudaProviderOptions">Native OrtCUDAProviderOptions instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_CUDA_V2">
            <summary>
            Append a CUDA EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="cudaProviderOptions">Native OrtCUDAProviderOptionsV2 instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_ROCM">
            <summary>
            Append a ROCm EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="rocmProviderOptions">Native OrtROCMProviderOptions instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddFreeDimensionOverride">
            <summary>
            Free Dimension override (by denotation)
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="dimDenotation">Dimension denotation</param>
            <param name="dimValue">Dimension value</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddFreeDimensionOverrideByName">
            <summary>
            Free Dimension override (by name)
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="dimName">Dimension name</param>
            <param name="dimValue">Dimension value</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtRegisterCustomOpsLibrary">
            <summary>
            Register custom op library
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="libraryPath">Library path</param>
            <param name="libraryHandle">(out) Native library handle</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtRegisterCustomOpsLibrary_V2">
            <summary>
            Register custom op library. ORT will manage freeing the library.
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="libraryPath">Library path</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddInitializer">
            <summary>
            Add initializer that is shared across Sessions using this SessionOptions (by denotation)
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="name">Name of the initializer</param>
            <param name="ortValue">Native OrtValue instnce</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider">
             <summary>
             Append an execution provider instance to the native OrtSessionOptions instance.
            
             'SNPE' and 'XNNPACK' are currently supported as providerName values.
            
             The number of providerOptionsKeys must match the number of providerOptionsValues and equal numKeys.
             </summary>
             <param name="options">Native OrtSessionOptions instance</param>
             <param name="providerName">Execution provider to add.</param>
             <param name="providerOptionsKeys">Configuration keys to add</param>
             <param name="providerOptionsValues">Configuration values to add</param>
             <param name="numKeys">Number of configuration keys</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddRunConfigEntry">
            <summary>
            Add run config entry
            </summary>
            <param name="options">Native RunOptions instance</param>
            <param name="configKey">Config key</param>
            <param name="configValue">Config value</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtMemoryInfoGetName">
            Do not free the returned value
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateArenaCfg">
            <summary>
            Create an instance of arena configuration which will be used to create an arena based allocator
            See docs/C_API.md for details on what the following parameters mean and how to choose these values
            </summary>
            <param name="maxMemory">Maximum amount of memory the arena allocates</param>
            <param name="arenaExtendStrategy">Strategy for arena expansion</param>
            <param name="initialChunkSizeBytes">Size of the region that the arena allocates first</param>
            <param name="maxDeadBytesPerChunk">Maximum amount of fragmentation allowed per chunk</param>
            <returns>Pointer to a native OrtStatus instance indicating success/failure of config creation</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseArenaCfg">
            <summary>
            Destroy an instance of an arena configuration instance
            </summary>
            <param name="arenaCfg">arena configuration instance to be destroyed</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateAllocator">
            <summary>
            Create an instance of allocator according to mem_info
            </summary>
            <param name="session">Session that this allocator should be used with</param>
            <param name="info">memory allocator specs</param>
            <param name="allocator">out pointer to a new allocator instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseAllocator">
            <summary>
            Destroy an instance of an allocator created by OrtCreateAllocator
            </summary>
            <param name="allocator">instance to be destroyed</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAllocatorAlloc">
            <summary>
            Allocate  a chunk of native memory
            </summary>
            <param name="allocator">allocator instance</param>
            <param name="size">bytes to allocate</param>
            <param name="p">out pointer to the allocated memory. Must be freed by OrtAllocatorFree</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAllocatorFree">
            <summary>
            Release native memory allocated by an allocator
            </summary>
            <param name="allocator">allocator instance</param>
            <param name="p">pointer to native memory allocated by the allocator instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateIoBinding">
            <summary>
            Create OrtIoBinding instance that is used to bind memory that is allocated
            either by a 3rd party allocator or an ORT device allocator. Such memory should be wrapped by
            a native OrtValue of Tensor type. By binding such named values you will direct ORT to read model inputs
            and write model outputs to the supplied memory.
            </summary>
            <param name="session">session to create OrtIoBinding instance</param>
            <param name="io_binding">out a new instance of OrtIoBinding</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseIoBinding">
            <summary>
            Destroy OrtIoBinding instance created by OrtCreateIoBinding
            </summary>
            <param name="io_bidning">instance of OrtIoBinding</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtBindInput">
            <summary>
            Bind OrtValue to the model input with the specified name
            If binding with the specified name already exists, it will be replaced
            </summary>
            <param name="io_bidning">instance of OrtIoBinding</param>
            <param name="name">model input name (utf-8)</param>
            <param name="ort_value">OrtValue that is used for input (may wrap arbitrary memory).
                 The param instance is copied internally so this argument may be released.
            </param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtSynchronizeBoundInputs">
            <summary>
            The API calls Sync() on all EP providers present. This blocks until the device has completed
            all preceding requested tasks. This is necessary when memory synchronization is required.
            For example, the memory bound to an input is likely to be on a different CUDA stream.
            For some scenarios and devices this may be a no-op, use
            your best judgment.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
            <returns>An instance of OrtStatus or null</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtBindOutput">
            <summary>
            Bind OrtValue to the model output with the specified name
            If binding with the specified name already exists, it will be replaced
            </summary>
            <param name="io_bidning">instance of OrtIoBinding</param>
            <param name="name">model output name (utf-8)</param>
            <param name="ort_value">OrtValue that is used for output (may wrap arbitrary memory).
                 The param instance is copied internally so this argument may be released.
            </param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtBindOutputToDevice">
            <summary>
            Bind a device to the model output with the specified name
            This is useful when the OrtValue can not be allocated ahead of time
            due to unknown dimensions.
            </summary>
            <param name="io_binding">Instance of OrtIoBinding</param>
            <param name="name">UTF-8 zero terminated name</param>
            <param name="mem_info">OrtMemoryInfo instance that contains device id. May be obtained from the device specific allocator instance</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtSynchronizeBoundOutputs">
            <summary>
            The API calls Sync() on all EP providers present. This blocks until the device has completed
            all preceding requested tasks. This is necessary when memory synchronization is required.
            For some scenarios and devices this may be a no-op, use your best judgment.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
            <returns>An instance of OrtStatus or null</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetBoundOutputNames">
            <summary>
            The function will return all bound output names in the order they were bound.
            It is the same order that the output values will be returned after RunWithBinding() is used.
            The function will allocate two native allocations  using the allocator supplied.
            The caller is responsible for deallocating both of the buffers using the same allocator.
            You may use OrtMemoryAllocation disposable class to wrap those allocations.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
            <param name="allocator">allocator to use for memory allocation</param>
            <param name="buffer">a continuous buffer that contains all output names.
            Names are not zero terminated use lengths to extract strings. This needs to be deallocated.</param>
            <param name="lengths">A buffer that contains lengths (size_t) for each of the returned strings in order.
            The buffer must be deallocated.</param>
            <param name="count">this contains the count of names returned which is the number of elements in lengths.</param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetBoundOutputValues" -->
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtClearBoundInputs">
            <summary>
            Clears Input bindings. This is a convenience method.
            Releasing OrtIoBinding instance would clear all bound inputs.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtClearBoundOutputs">
            <summary>
            Clears Output bindings. This is a convenience method.
            Releasing OrtIoBinding instance would clear all bound outputs.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtTensorAt">
            <summary>
            Provides element-level access into a tensor.
            </summary>
            <param name="location_values">a pointer to an array of index values that specify an element's location in the tensor data blob</param>
            <param name="location_values_count">length of location_values</param>
            <param name="out">a pointer to the element specified by location_values</param>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateAndRegisterAllocator" -->
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtSetLanguageProjection">
            <summary>
            Set the language projection for collecting telemetry data when Env is created
            </summary>
            <param name="projection">the source projected language</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtSessionGetModelMetadata">
            <summary>
            Gets the ModelMetadata associated with an InferenceSession
            </summary>
            <param name="session">instance of OrtSession</param>
            <param name="modelMetadata">(output) instance of OrtModelMetadata</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetProducerName">
            <summary>
            Gets the producer name associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) producer name from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetGraphName">
            <summary>
            Gets the graph name associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) graph name from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetDomain">
            <summary>
            Gets the domain associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) domain from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetDescription">
            <summary>
            Gets the description associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) description from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetGraphDescription">
            <summary>
            Gets the description associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) graph description from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetVersion">
            <summary>
            Gets the version associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="value">(output) version from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetCustomMetadataMapKeys">
            <summary>
            Gets all the keys in the custom metadata map in the ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="keys">(output) all keys in the custom metadata map</param>
            <param name="numKeys">(output) number of keys in the custom metadata map</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataLookupCustomMetadataMap">
            <summary>
            Gets the value associated with the given key in custom metadata map in the ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="key">key in the custom metadata map</param>
            <param name="value">(output) value for the key in the custom metadata map</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseModelMetadata">
            <summary>
            Frees ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetTensorMutableData">
            This function doesn't work with string tensor
            this is a no-copy method whose pointer is only valid until the backing OrtValue* is free'd.
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtFillStringTensor">
            \param value A tensor created from OrtCreateTensor... function.
            \param len total data length, not including the trailing '\0' chars.
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetResizedStringTensorElementBuffer" -->
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetSymbolicDimensions">
            Get the symbolic dimension names for dimensions with a value of -1.
            Order and number of entries is the same as values returned by GetDimensions.
            The name may be empty for an unnamed symbolic dimension.
            e.g.
            If OrtGetDimensions returns [-1, -1, 2], OrtGetSymbolicDimensions would return an array with 3 entries.
            If the values returned were ['batch', '', ''] it would indicate that
             - the first dimension was a named symbolic dimension (-1 dim value and name in symbolic dimensions),
             - the second dimension was an unnamed symbolic dimension (-1 dim value and empty string),
             - the entry for the third dimension should be ignored as it is not a symbolic dimension (dim value >= 0).
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetTensorShapeElementCount">
            How many elements does this tensor have.
            May return a negative value
            e.g.
            [] -> 1
            [1,3,4] -> 12
            [2,0,4] -> 0
            [-1,3,4] -> -1
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DCastTypeInfoToMapTypeInfo">
             Map Type API
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetAvailableProviders">
            <summary>
            Queries all the execution providers supported in the native onnxruntime shared library
            </summary>
            <param name="providers">(output) all execution providers (strings) supported in the native onnxruntime shared library</param>
            <param name="numProviders">(output) number of execution providers (strings)</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseAvailableProviders">
            <summary>
            Releases all execution provider strings allocated and returned by OrtGetAvailableProviders
            </summary>
            <param name="providers">all execution providers (strings) returned by OrtGetAvailableProviders</param>
            <param name="numProviders">number of execution providers (strings)</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreatePrepackedWeightsContainer">
            <summary>
            Create an instance of PrepackedWeightsContainer
            </summary>
            <param name="prepackedWeightsContainer">(output) Created native OrtPrepackedWeightsContainer instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleasePrepackedWeightsContainer">
            <summary>
            Destroy an instance of PrepackedWeightsContainer
            </summary>
            <param name="prepackedWeightsContainer">Native OrtPrepackedWeightsContainer instance to be destroyed</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper">
            <summary>
            This helper class contains methods to create native OrtValue from a managed value object
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.StringToZeroTerminatedUtf8(System.String)">
            <summary>
            Converts C# UTF-16 string to UTF-8 zero terminated
            byte[] instance
            </summary>
            <param name="s">string to be converted</param>
            <returns>UTF-8 encoded equivalent</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.StringToUtf8NativeMemory(System.Char*,System.Int32,System.IntPtr,System.Int32)">
            <summary>
            This function converts the input string into UTF-8 encoding string (no zero termination)
            straight into the pre-allocated native buffer. The buffer size
            must match the required size and can be obtained in advance with
            System.Text.Encoding.UTF8.GetByteCount(s).
            
            </summary>
            <param name="strPtr">fixed char* ptr</param>
            <param name="strLength">string length</param>
            <param name="ptr">Native buffer to write</param>
            <param name="nativeBufferSize"></param>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.StringFromNativeUtf8(System.IntPtr,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            Reads UTF-8 encode string from a C zero terminated string
            and converts it into a C# UTF-16 encoded string
            </summary>
            <param name="nativeUtf8">pointer to native or pinned memory where Utf-8 resides</param>
            <param name="allocator">optional allocator to free nativeUtf8 if it was allocated by OrtAllocator</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.StringAndUtf8FromNative(Microsoft.ML.OnnxRuntime.OrtAllocator,System.IntPtr,System.String@,System.IntPtr@)">
            <summary>
            Reads UTF-8 string from native C zero terminated string,
            makes a copy of it on unmanaged heap and converts it to C# UTF-16 string,
            then returns both C# string and the unmanaged copy of the UTF-8 string.
            
            On return it deallocates the nativeUtf8 string using the specified allocator
            </summary>
            <param name="allocator">allocator to use to free nativeUtf8</param>
            <param name="nativeUtf8">input</param>
            <param name="str">C# UTF-16 string</param>
            <param name="utf8">UTF-8 bytes in a unmanaged allocation, zero terminated</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.GetPlatformSerializedString(System.String)">
            <summary>
            Converts C# UTF-16 string to UTF-8 zero terminated
            byte[] instance
            </summary>
            <param name="str">string to be converted</param>
            <returns>UTF-8 encoded equivalent</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.MarshaledString">
            <summary>
            This class converts a string to a UTF8 encoded byte array and then copies it to an unmanaged buffer.
            This is done, so we can pass it to the native code and avoid pinning.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Microsoft.ML.OnnxRuntime.MarshaledString.Length" -->
        <member name="P:Microsoft.ML.OnnxRuntime.MarshaledString.Value">
            <summary>
            Actual native buffer
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.MarshaledString.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.MarshaledStringArray">
            <summary>
            Keeps a list of MarshaledString instances and provides a way to dispose them all at once.
            It is a ref struct, so it can not be IDisposable.
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ProviderOptionsUpdater">
            <summary>
            Utility class used in SessioniOptions and ProviderOptions
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ProviderOptionsUpdater.Update(System.Collections.Generic.Dictionary{System.String,System.String},System.IntPtr,System.Func{System.IntPtr,System.IntPtr[],System.IntPtr[],System.UIntPtr,System.IntPtr})">
            <summary>
            A utility method to update the provider options, provides common functionality.
            
            </summary>
            <param name="providerOptions">The actual key/value option pairs</param>
            <param name="handle">to the object</param>
            <param name="updateFunc">encapsulates a native method that returns 
            Arg1=handle, Arg2=array of keys, Arg3=array of values, Arg4 - count, Arg5 - return ORT status</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtAllocatorType">
            <summary>
            See documentation for OrtAllocatorType in C API
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtMemType">
            <summary>
            See documentation for OrtMemType in C API
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtArenaCfg">
            <summary>
            This class encapsulates arena configuration information that will be used to define the behavior
            of an arena based allocator
            See docs/C_API.md for more details
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtArenaCfg.#ctor(System.UInt32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Create an instance of arena configuration which will be used to create an arena based allocator
            See docs/C_API.md for details on what the following parameters mean and how to choose these values
            </summary>
            <param name="maxMemory">Maximum amount of memory the arena allocates</param>
            <param name="arenaExtendStrategy">Strategy for arena expansion</param>
            <param name="initialChunkSizeBytes">Size of the region that the arena allocates first</param>
            <param name="maxDeadBytesPerChunk">Maximum amount of fragmentation allowed per chunk</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtArenaCfg.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtArenaCfg.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtEnv
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtMemoryInfo">
            <summary>
            This class encapsulates and most of the time owns the underlying native OrtMemoryInfo instance.
            Instance returned from OrtAllocator will not own OrtMemoryInfo, the class must be disposed
            regardless.
            
            Use this class to query and create OrtAllocator instances so you can pre-allocate memory for model
            inputs/outputs and use it for binding. Instances of the class can also used to created OrtValues bound
            to pre-allocated memory. In that case, the instance of OrtMemoryInfo contains the information about the allocator
            used to allocate the underlying memory.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.DefaultInstance">
            <summary>
            Default CPU based instance
            </summary>
            <value>Singleton instance of a CpuMemoryInfo</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.#ctor(System.IntPtr,System.Boolean)">
            <summary>
            This allocator takes an native pointer to already existing
            instance of OrtMemoryInfo. That instance may either be owned or not
            owned. In the latter case, this class serves to expose native properties
            of the instance.
            </summary>
            <param name="allocInfo"></param>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.allocatorCPU">
            <summary>
            Predefined utf8 encoded allocator names. Use them to construct an instance of
            OrtMemoryInfo to avoid UTF-16 to UTF-8 conversion costs.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.allocatorCUDA">
            <summary>
            Predefined utf8 encoded allocator names. Use them to construct an instance of
            OrtMemoryInfo to avoid UTF-16 to UTF-8 conversion costs.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.allocatorCUDA_PINNED">
            <summary>
            Predefined utf8 encoded allocator names. Use them to construct an instance of
            OrtMemoryInfo to avoid UTF-16 to UTF-8 conversion costs.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.allocatorHIP">
            <summary>
            Predefined utf8 encoded allocator names. Use them to construct an instance of
            OrtMemoryInfo to avoid UTF-16 to UTF-8 conversion costs.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.allocatorHIP_PINNED">
            <summary>
            Predefined utf8 encoded allocator names. Use them to construct an instance of
            OrtMemoryInfo to avoid UTF-16 to UTF-8 conversion costs.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.#ctor(System.Byte[],Microsoft.ML.OnnxRuntime.OrtAllocatorType,System.Int32,Microsoft.ML.OnnxRuntime.OrtMemType)">
            <summary>
            Create an instance of OrtMemoryInfo according to the specification
            Memory info instances are usually used to get a handle of a native allocator
            that is present within the current inference session object. That, in turn, depends
            of what execution providers are available within the binary that you are using and are
            registered with Add methods.
            </summary>
            <param name="utf8AllocatorName">Allocator name. Use of the predefined above.</param>
            <param name="allocatorType">Allocator type</param>
            <param name="deviceId">Device id</param>
            <param name="memoryType">Memory type</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.#ctor(System.String,Microsoft.ML.OnnxRuntime.OrtAllocatorType,System.Int32,Microsoft.ML.OnnxRuntime.OrtMemType)">
            <summary>
            Create an instance of OrtMemoryInfo according to the specification.
            </summary>
            <param name="allocatorName">Allocator name</param>
            <param name="allocatorType">Allocator type</param>
            <param name="deviceId">Device id</param>
            <param name="memoryType">Memory type</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.Name">
            <summary>
            Name of the allocator associated with the OrtMemoryInfo instance
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.Id">
            <summary>
            Returns device ID
            </summary>
            <value>returns integer Id value</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.GetMemoryType">
            <summary>
             The below 2 are really properties but naming them is a challenge
             as names would conflict with the returned type. Also, there are native
             calls behind them so exposing them as Get() would be appropriate.
            </summary>
            <returns>OrtMemoryType for the instance</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.GetAllocatorType">
            <summary>
            Fetches allocator type from the underlying OrtAllocator
            </summary>
            <returns>Returns allocator type</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.Equals(System.Object)">
            <summary>
            Overrides System.Object.Equals(object)
            </summary>
            <param name="obj">object to compare to</param>
            <returns>true if obj is an instance of OrtMemoryInfo and is equal to this</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.Equals(Microsoft.ML.OnnxRuntime.OrtMemoryInfo)">
            <summary>
            Compares this instance with another
            </summary>
            <param name="other">OrtMemoryInfo to compare to</param>
            <returns>true if instances are equal according to OrtCompareMemoryInfo.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.GetHashCode">
            <summary>
            Overrides System.Object.GetHashCode()
            </summary>
            <returns>integer hash value</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtMmeoryInfo
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtExternalAllocation">
            <summary>
            This class represents an arbitrary buffer of memory
            allocated and owned by the user. It can be either a CPU, GPU or other device memory
            that can be suitably represented by IntPtr.
            This is just a composite of the buffer related information.
            The memory is assumed to be pinned if necessary and usable immediately
            in the native code.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.#ctor(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,System.Int64[],Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.IntPtr,System.Int64)">
            <summary>
            Constructor
            </summary>
            <param name="memInfo">use to accurately describe a piece of memory that this is wrapping</param>
            <param name="shape">shape of this buffer</param>
            <param name="elementType">element type</param>
            <param name="pointer">the actual pointer to memory</param>
            <param name="sizeInBytes">size of the allocation in bytes</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.Info">
            <summary>
            OrtMemoryInfo
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.Shape">
            <summary>
            Shape
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.ElementType">
            <summary>
            Data type
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.Pointer">
            <summary>
            Actual memory ptr
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.Size">
            <summary>
            Size of the allocation in bytes
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation">
            <summary>
            This class represents memory allocation made by a specific onnxruntime
            allocator. Use OrtAllocator.Allocate() to obtain an instance of this class.
            It implements IDisposable and makes use of the original allocator
            used to allocate the memory. The lifespan of the allocator instance must eclipse the
            lifespan of the allocation. Or, if you prefer, all OrtMemoryAllocation instances must be
            disposed of before the corresponding allocator instances are disposed of.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.#ctor(Microsoft.ML.OnnxRuntime.OrtAllocator,System.IntPtr,System.UInt32)">
            <summary>
            This constructs an instance representing an native memory allocation.
            Typically returned by OrtAllocator.Allocate(). However, some APIs return
            natively allocated IntPtr using a specific allocator. It is a good practice
            to wrap such a memory into OrtAllocation for proper disposal. You can set
            size to zero if not known, which is not important for disposing.
            </summary>
            <param name="allocator"></param>
            <param name="pointer"></param>
            <param name="size"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.Pointer">
            <summary>
            Internal accessor to call native methods
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.Size">
            <summary>
            Size of the allocation
            </summary>
            <value>uint size of the allocation in bytes</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.Info">
            <summary>
            Memory Information about this allocation
            </summary>
            <value>Returns OrtMemoryInfo from the allocator</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to deallocate
            a chunk of memory using the specified allocator.
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtAllocator">
            <summary>
            The class exposes native internal allocator for Onnxruntime.
            This allocator enables you to allocate memory from the internal
            memory pools including device allocations. Useful for binding.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtAllocator.DefaultInstance">
            <summary>
            Default CPU allocator instance
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtAllocator.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.#ctor(System.IntPtr,System.Boolean)">
            <summary>
            Internal constructor wraps existing native allocators
            </summary>
            <param name="allocator"></param>
            <param name="owned"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.#ctor(Microsoft.ML.OnnxRuntime.InferenceSession,Microsoft.ML.OnnxRuntime.OrtMemoryInfo)">
            <summary>
            Creates an instance of OrtAllocator according to the specifications in OrtMemorInfo.
            The requested allocator should be available within the given session instance. This means
            both, the native library was build with specific allocators (for instance CUDA) and the corresponding
            provider was added to SessionsOptions before instantiating the session object.
            </summary>
            <param name="session"></param>
            <param name="memInfo"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtAllocator.Info">
            <summary>
            OrtMemoryInfo instance owned by the allocator
            </summary>
            <value>Instance of OrtMemoryInfo describing this allocator</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.Allocate(System.UInt32)">
            <summary>
            Allocate native memory. Returns a disposable instance of OrtMemoryAllocation
            </summary>
            <param name="size">number of bytes to allocate</param>
            <returns>Instance of OrtMemoryAllocation</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.FreeMemory(System.IntPtr)">
            <summary>
            This internal interface is used for freeing memory.
            </summary>
            <param name="allocation">pointer to a native memory chunk allocated by this allocator instance</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtAllocator
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.DOrtLoggingFunction">
            <summary>
            Delegate for logging function callback.
            Supply your function and register it with the environment to receive logging callbacks via
            EnvironmentCreationOptions
            </summary>
            <param name="param">Pointer to data passed into Constructor `log_param` parameter.</param>
            <param name="severity">Log severity level.</param>
            <param name="category">Log category</param>
            <param name="logId">Log Id.</param>
            <param name="codeLocation">Code location detail.</param>
            <param name="message">Log message.</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions">
            <summary>
            Options you might want to supply when creating the environment.
            Everything is optional.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.logId">
            <summary>
             Supply a log id to identify the application using ORT, otherwise, a default one will be used
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.logLevel">
            <summary>
            Initial logging level so that you can see what is going on during environment creation
            Default is LogLevel.Warning
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.threadOptions">
            <summary>
            Supply OrtThreadingOptions instance, otherwise null
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.loggingParam">
            <summary>
            Supply IntPtr logging param when registering logging function, otherwise IntPtr.Zero
            This param will be passed to the logging function when called, it is opaque for the API
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.loggingFunction">
            <summary>
            Supply custom logging function otherwise null
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtEnv">
            <summary>
            The singleton class OrtEnv contains the process-global ONNX Runtime environment.
            It sets up logging, creates system wide thread-pools (if Thread Pool options are provided)
            and other necessary things for OnnxRuntime to function. 
            
            Create or access OrtEnv by calling the Instance() method. Instance() can be called multiple times.
            It would return the same instance.
            
            CreateInstanceWithOptions() provides a way to create environment with options.
            It must be called once before Instance() is called, otherwise it would not have effect.
            
            If the environment is not explicitly created, it will be created as needed, e.g.,
            when creating a SessionOptions instance.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.#ctor(System.IntPtr,Microsoft.ML.OnnxRuntime.OrtLoggingLevel)">
            <summary>
            The only __ctor__ for OrtEnv.
            </summary>
            <param name="handle"></param>
            <param name="logLevel"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.LoggingFunctionThunk(System.IntPtr,System.IntPtr,System.IntPtr,System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            The actual logging callback to the native code
            </summary>
            <param name="param"></param>
            <param name="severity"></param>
            <param name="category"></param>
            <param name="logid"></param>
            <param name="code_location"></param>
            <param name="message"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.CreateInstance">
            <summary>
            This is invoked only once when the first call refers _instance.Value.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.SetLanguageProjection(Microsoft.ML.OnnxRuntime.OrtEnv)">
            <summary>
            To be called only from constructor
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.Instance">
            <summary>
            Instantiates (if not already done so) a new OrtEnv instance with the default logging level
            and no other options. Otherwise returns the existing instance.
            
            It returns the same instance on every call - `OrtEnv` is singleton
            </summary>
            <returns>Returns a singleton instance of OrtEnv that represents native OrtEnv object</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.CreateInstanceWithOptions(Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions@)">
            <summary>
            Provides a way to create an instance with options.
            It throws if the instance already exists and the specified options
            not have effect.
            </summary>
            <param name="options"></param>
            <returns></returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException">if the singleton has already been created</exception>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtEnv.IsCreated">
            <summary>
            Provides visibility if singleton already been instantiated
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.EnableTelemetryEvents">
            <summary>
            Enable platform telemetry collection where applicable
            (currently only official Windows ORT builds have telemetry collection capabilities)
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.DisableTelemetryEvents">
            <summary>
            Disable platform telemetry collection
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.CreateAndRegisterAllocator(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,Microsoft.ML.OnnxRuntime.OrtArenaCfg)">
            <summary>
            Create and register an allocator to the OrtEnv instance
            so as to enable sharing across all sessions using the OrtEnv instance
            <param name="memInfo">OrtMemoryInfo instance to be used for allocator creation</param>
            <param name="arenaCfg">OrtArenaCfg instance that will be used to define the behavior of the arena based allocator</param>
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.GetVersionString">
            <summary>
            This function returns the onnxruntime version string
            </summary>
            <returns>version string</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.GetAvailableProviders">
            <summary>
            Queries all the execution providers supported in the native onnxruntime shared library
            </summary>
            <returns>an array of strings that represent execution provider names</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtEnv.EnvLogLevel">
            <summary>
            Get/Set log level property of OrtEnv instance
            Default LogLevel.Warning
            </summary>
            <returns>env log level</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtEnv.Handle">
            <summary>
            Returns a handle to the native `OrtEnv` instance held by the singleton C# `OrtEnv` instance
            Exception caching: May throw an exception on every call, if the `OrtEnv` constructor threw an exception
            during lazy initialization
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtEnv.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.ReleaseHandle">
            <summary>
            Destroys native object
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BitOpsUtils.LeadingZeroCount(System.UInt32)">
            <summary>
            Required because BitOperations are not available in NETSTANDARD 2.0.
            There are more efficient ways with bit twiddling, but this one has clarity.
            </summary>
            <param name="num">value</param>
            <returns>number of leading zeros. Useful to compute log2 as well.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BitOpsUtils.SingleToUInt32Bits(System.Single)">
            <summary>
            Extracts single precision number bit representation as uint
            so its bits can be manipulated.
            
            This API is the reverse of UInt32BitsToSingle().
            
            </summary>
            <param name="single">float value</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BitOpsUtils.UInt32BitsToSingle(System.UInt32)">
            <summary>
            Needed because BitConverter impl is not available until
            later versions. This API is the reverse of SingleToUInt32Bits().
            
            For the exact bit representation of float see IEEE 754 standard for single precision.
            
            </summary>
            <param name="singleBits">bit representation of float either obtained from 
            SingleToUInt32Bits or assembled using bitwise operators</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BitOpsUtils.SingleBitsToBFloat16Bits(System.UInt32)">
            <summary>
            Converts single precision bits representation which can be obtained using
            SingleToUInt32Bits() or manually constructed according to IEEE 754 standard.
            
            </summary>
            <param name="singleBits">bits representation of a single precision number (float)</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BitOpsUtils.BFloat16BitsToSingleBits(System.UInt16)">
            <summary>
            Converts bfloat16 ushort bits representation to single precision bits which then in turn can be
            manipulated or converted to float using UInt32BitsToSingle()
            </summary>
            <param name="bfloatBits">ushort bits representation of bfloat16</param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.BitOpsUtils.CreateSingleNaN(System.Boolean,System.UInt64)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.BitOpsUtils.CreateSingle(System.Boolean,System.Byte,System.UInt32)">
            <summary>
            Creates float from sign, exponent and significand
            </summary>
            <param name="sign">true if negative</param>
            <param name="exponent">exponent</param>
            <param name="significand">significand</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Float16">
            <summary>
            This value type represents A Float16 value
            it is blittable as defined in https://docs.microsoft.com/en-us/dotnet/framework/interop/blittable-and-non-blittable-types
            and as such, represented the same way in managed and native memories. This means that arrays of this type
            do not have to be copied to be passed to native memory but simply pinned and read by native code. Thus,
            one can create a Tensor on top of an array of these structures and feed it directly to Onnxruntime library.
            Binary wise, it is the same as ushort[] (uint16_t in C++). However, we would like a separate type for type dispatching.
            
            The implementation is derived from 
            https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/Half.cs,7895d5942d33f974
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.Epsilon">
            <summary>
            Float16 Epsilon value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.Pi">
            <summary>
            Float16 Pi value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.PositiveInfinity">
            <summary>
            Float16 Positive Infinity value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.NegativeInfinity">
            <summary>
            Float16 Negative Infinity value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.NaN">
            <summary>
            Float16 NaN
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.Zero">
            <summary>
            Float16 Zero value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.One">
            <summary>
            Float16 One value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.NegativeZero">
            <summary>
            Float16 Negative Zero value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.MinValue">
            <summary>
            Float16 Min value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Float16.MaxValue">
            <summary>
            Float16 Max value
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.Float16.value">
            <summary>
            float16 representation bits
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.#ctor(System.UInt16)">
            <summary>
            Ctor from ushort bits, no conversion is done
            </summary>
            <param name="v"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.op_LessThan(Microsoft.ML.OnnxRuntime.Float16,Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Compares values of two Float16
            
            </summary>
            <param name="left">left hand side</param>
            <param name="right">right hand side</param>
            <returns>returns true if left is less than right according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.op_GreaterThan(Microsoft.ML.OnnxRuntime.Float16,Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Compares values of two Float16
            
            </summary>
            <param name="left">left hand side</param>
            <param name="right">right hand side</param>
            <returns>returns true if left is greater than right according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.op_LessThanOrEqual(Microsoft.ML.OnnxRuntime.Float16,Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Compares values of two Float16
            
            </summary>
            <param name="left">left hand side</param>
            <param name="right">right hand side</param>
            <returns>returns true if left is less or equal than right according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.op_GreaterThanOrEqual(Microsoft.ML.OnnxRuntime.Float16,Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Compares values of two Float16
            
            </summary>
            <param name="left">left hand side</param>
            <param name="right">right hand side</param>
            <returns>returns true if left is greater or equal than right according to IEEE</returns>
            <inheritdoc cref="!:IComparisonOperators&lt;TSelf, TOther, TResult&gt;.op_GreaterThanOrEqual(TSelf, TOther)" />
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.op_Equality(Microsoft.ML.OnnxRuntime.Float16,Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Compares values of two Float16 for binary equality.
            If either of the values is NaN, this will return false.
            
            </summary>
            <param name="left">left hand side</param>
            <param name="right">right hand side</param>
            <returns>true if values are equal according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.op_Inequality(Microsoft.ML.OnnxRuntime.Float16,Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Compares values of two Float16 for binary inequality
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns>true if values are not equal according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.IsFinite(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Determines whether the specified value is finite (zero, subnormal, or normal).
            </summary>
            <param name="value">Float16 instance.</param>
            <returns>true if the value is finite</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.IsInfinity(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Determines whether the specified value is infinite.
            </summary>
            <param name="value">Float16 instance.</param>
            <returns>true if the value is infinite</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.IsNaN(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Determines whether the specified value is NaN.
            </summary>
            
            <param name="value">Float16 instance</param>
            <returns>true if the value is not a number</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.Float16.IsNegative(Microsoft.ML.OnnxRuntime.Float16)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.IsNegativeInfinity(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Determines whether the specified value is negative infinity.
            </summary>
            
            <param name="value">Float16 instance</param>
            <returns>true if the value is negative infinity</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.IsNormal(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Determines whether the specified value is normal
            </summary>
            <param name="value"></param>
            <returns>true or false</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.IsPositiveInfinity(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Determines whether the specified value is positive infinity.
            </summary>
            <param name="value">Float16 instance</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.IsSubnormal(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Determines whether the specified value is subnormal.
            </summary>
            <param name="value">Float16 instance</param>
            <returns>true if the value is subnormal</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.CompareTo(System.Object)">
            <summary>
            Compares this object to another object, returning an integer that indicates the relationship.
            </summary>
            
            <param name="obj">Object to compare to</param>
            <returns>A value less than zero if this is less than <paramref name="obj"/>,
            zero if this is equal to <paramref name="obj"/>, or a value greater than zero
            if this is greater than <paramref name="obj"/>.
            </returns>
            <exception cref="T:System.ArgumentException">Thrown when <paramref name="obj"/> is not of type <see cref="T:Microsoft.ML.OnnxRuntime.Float16"/>.</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.CompareTo(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Compares this object to another object, returning an integer that indicates the relationship.
            </summary>
            <param name="other">Object to compare to</param>
            <returns>A value less than zero if this is less than <paramref name="other"/>,
            zero if this is equal to <paramref name="other"/>, 
            or a value greater than zero if this is greater than <paramref name="other"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.Equals(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Returns a value indicating whether this instance and other Float16 represent the same value.
            </summary>
            <param name="other">A Float16 object to compare to this instance.</param>
            <returns>true if other.value is equal to this instance; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.Equals(System.Object)">
            <summary>
            Returns a value indicating whether this instance and a specified System.Object
            represent the same type and value.
            </summary>
            <param name="obj">An System.Object.</param>
            <returns>true if obj is Float16 and its value is equal to this instance; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.GetHashCode">
            <summary>
            Returns the hash code for this instance.
            </summary>
            <returns>A 32-bit signed integer hash code.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.ToString">
            <summary>
            Returns a string representation of the current value.
            </summary>
            <returns>Text representation of Float16</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.ToFloat">
            <summary>
            Explicit conversion
            </summary>
            <returns>single precision value converted from Float16</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.op_Explicit(System.Single)~Microsoft.ML.OnnxRuntime.Float16">
            <summary>Explicitly converts a <see cref="T:System.Single" /> value to its nearest representable half-precision floating-point value.</summary>
            <param name="value">The value to convert.</param>
            <returns><paramref name="value" /> converted to its nearest representable half-precision floating-point value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.op_Explicit(Microsoft.ML.OnnxRuntime.Float16)~System.Single">
            <summary>Explicitly converts a half-precision floating-point value to its nearest representable <see cref="T:System.Single" /> value.</summary>
            <param name="value">The value to convert.</param>
            <returns><paramref name="value" /> converted to its nearest representable <see cref="T:System.Single" /> value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.Negate(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            Flips the sign. NaNs are not affected.
            IEEE 754 specifies NaNs to be propagated
            </summary>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Float16.IsNaNOrZero(Microsoft.ML.OnnxRuntime.Float16)">
            <summary>
            The function returns true if the value is either NaN or zero.
            </summary>
            <param name="value">instance of Float16</param>
            <returns>true if NaN or zero.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.BFloat16">
            <summary>
            This value type represents A BFloat16 value.
            See https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus
            for details.
            it is blittable as defined in https://docs.microsoft.com/en-us/dotnet/framework/interop/blittable-and-non-blittable-types
            and as such, represented the same way in managed and native memories. This means that arrays of this type
            do not have to be copied to be passed to native memory but simply pinnned and read by native code. Thus,
            one can create a Tensor on top of an array of these structures and feed it directly to Onnxruntime library.
            Binary wise, it is the same as ushort[] (uint16_t in C++). However, we would like a separate type for type dispatching.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.Epsilon">
            <summary>
            BFloat16 Epsilon value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.Pi">
            <summary>
            BFloat16 Pi value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.PositiveInfinity">
            <summary>
            BFloat16 Positive infinity value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.NegativeInfinity">
            <summary>
            BFloat16 Negative infinity value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.NaN">
            <summary>
            BFloat16 NaN
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.Zero">
            <summary>
            BFloat16 Positive Zero
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.One">
            <summary>
            BFloat16 One
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.NegativeZero">
            <summary>
            BFloat16 Negative Zero
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.MinValue">
            <summary>
            BFloat16 Min value
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.BFloat16.MaxValue">
            <summary>
            BFloat16 Max value
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.BFloat16.value">
            <summary>
            bfloat16 representation bits
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.#ctor(System.UInt16)">
            <summary>
            Constructor from ushort, no conversion takes place. The value
            is assumed to be converted
            </summary>
            <param name="v">bfloat16 representation bits</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.op_LessThan(Microsoft.ML.OnnxRuntime.BFloat16,Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Compares two BFloat16 instances.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns>true if the left is less than right according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.op_GreaterThan(Microsoft.ML.OnnxRuntime.BFloat16,Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Compares two BFloat16 instances.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns>true if the left is greater than right according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.op_LessThanOrEqual(Microsoft.ML.OnnxRuntime.BFloat16,Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Compares two BFloat16 instances.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns>true if the left is less or equal than right according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.op_GreaterThanOrEqual(Microsoft.ML.OnnxRuntime.BFloat16,Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Compares two BFloat16 instances.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns>true if the left is greater or equal than right according to IEEE</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.op_Equality(Microsoft.ML.OnnxRuntime.BFloat16,Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Compares values of two BFloat16 for binary equality.
            If either of the values is NaN, this will return false.
            
            </summary>
            <param name="left">left hand side</param>
            <param name="right">right hand side</param>
            <returns>result of value comparisons</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.op_Inequality(Microsoft.ML.OnnxRuntime.BFloat16,Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Compares values of two BFloat16 for binary inequality
            If either of the values is NaN it would return true.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns>result of value comparisons</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.IsFinite(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Determines whether the specified value is finite (zero, subnormal, or normal).
            </summary>
            <param name="value">BFloat16 instance.</param>
            <returns>true if the value is finite</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.IsInfinity(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Determines whether the specified value is infinite.
            </summary>
            <param name="value">BFloat16 instance.</param>
            <returns>true if the value is infinite</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.IsNaN(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Determines whether the specified value is NaN.
            </summary>
            
            <param name="value">BFloat16 instance</param>
            <returns>true if the value is not a number</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.BFloat16.IsNegative(Microsoft.ML.OnnxRuntime.BFloat16)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.IsNegativeInfinity(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Determines whether the specified value is negative infinity.
            </summary>
            
            <param name="value">BFloat16 instance</param>
            <returns>true if the value is negative infinity</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.IsNormal(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Determines whether the specified value is normal
            </summary>
            <param name="value"></param>
            <returns>true or false</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.IsPositiveInfinity(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Determines whether the specified value is positive infinity.
            </summary>
            <param name="value">BFloat16 instance</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.IsSubnormal(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Determines whether the specified value is subnormal.
            </summary>
            <param name="value">BFloat16 instance</param>
            <returns>true if the value is subnormal</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.CompareTo(System.Object)">
            <summary>
            Compares this object to another object, returning an integer that indicates the relationship.
            </summary>
            
            <param name="obj">Object to compare to</param>
            <returns>A value less than zero if this is less than <paramref name="obj"/>,
            zero if this is equal to <paramref name="obj"/>, or a value greater than zero
            if this is greater than <paramref name="obj"/>.
            </returns>
            <exception cref="T:System.ArgumentException">Thrown when <paramref name="obj"/> is not of type <see cref="T:Microsoft.ML.OnnxRuntime.BFloat16"/>.</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.CompareTo(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Compares this object to another object, returning an integer that indicates the relationship.
            </summary>
            <param name="other">Object to compare to</param>
            <returns>A value less than zero if this is less than <paramref name="other"/>,
            zero if this is equal to <paramref name="other"/>, 
            or a value greater than zero if this is greater than <paramref name="other"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.Equals(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Returns a value indicating whether this instance and other BFloat16 represent the same value.
            </summary>
            <param name="other">A BFloat16 object to compare to this instance.</param>
            <returns>true if other.value is equal to this instance; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.Equals(System.Object)">
            <summary>
            Returns a value indicating whether this instance and a specified System.Object
            represent the same type and value.
            </summary>
            <param name="obj">An System.Object.</param>
            <returns>true if obj is BFloat16 its value is equal to this instance; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.GetHashCode">
            <summary>
            Returns the hash code for this instance.
            </summary>
            <returns>A 32-bit signed integer hash code.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.ToString">
            <summary>
            Returns a string representation of the current value.
            </summary>
            <returns>Text representation of BFloat16</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.ToFloat">
            <summary>
            Explicit conversion
            </summary>
            <returns>single precision value converted from Float16</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.op_Explicit(System.Single)~Microsoft.ML.OnnxRuntime.BFloat16">
            <summary>Explicitly converts a <see cref="T:System.Single" /> value to its nearest representable bfloat16 value.</summary>
            <param name="value">The value to convert.</param>
            <returns><paramref name="value" /> converted to its nearest representable half-precision floating-point value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.op_Explicit(Microsoft.ML.OnnxRuntime.BFloat16)~System.Single">
            <summary>
            Explicitly converts a BFloat16 value to its nearest representable <see cref="T:System.Single" /> value.
            </summary>
            <param name="value">The value to convert.</param>
            <returns><paramref name="value" /> converted to its nearest representable <see cref="T:System.Single" /> value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.Negate(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            Flips the sign. NaNs are not affected.
            IEEE 754 specifies NaNs to be propagated
            </summary>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.BFloat16.IsNaNOrZero(Microsoft.ML.OnnxRuntime.BFloat16)">
            <summary>
            The function returns true if the value is either NaN or zero.
            </summary>
            <param name="value">instance of BFloat16</param>
            <returns>true if NaN or zero.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtIoBinding">
             <summary>
             This class enables binding of inputs and/or outputs to pre-allocated
             memory. This enables interesting scenarios. For example, if your input
             already resides in some pre-allocated memory like GPU, you can bind
             that piece of memory to an input name and shape and onnxruntime will use that as input.
             Other traditional inputs can also be bound that already exists as Tensors.
            
             Note, that this arrangement is designed to minimize data copies and to that effect
             your memory allocations must match what is expected by the model, whether you run on
             CPU or GPU. Data copy will still be made, if your pre-allocated memory location does not
             match the one expected by the model. However, copies with OrtIoBindings are only done once,
             at the time of the binding, not at run time. This means, that if your input data required a copy,
             your further input modifications would not be seen by onnxruntime unless you rebind it, even if it is
             the same buffer. If you require the scenario where data is copied, OrtIOBinding may not be the best match
             for your use case. The fact that data copy is not made during runtime also has performance implications.
             
             Making OrtValue first class citizen in ORT C# API practically obsoletes all of the existing overloads
             because OrtValue can be created on top of the all other types of memory. No need to designate it as external
             or Ort allocation or wrap it in FixedBufferOnnxValue. The latter does not support rebinding or memory other than
             CPU anyway.
             
             In fact, one can now create OrtValues over arbitrary pieces of memory, managed, native, stack and device(gpu)
             and feed them to the model and achieve the same effect without using IOBinding class.
             </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.#ctor(Microsoft.ML.OnnxRuntime.InferenceSession)">
            <summary>
            Use InferenceSession.CreateIoBinding()
            </summary>
            <param name="session"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtIoBinding.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInput(System.String,Microsoft.ML.OnnxRuntime.OrtValue)">
            <summary>
            This is the preferable and universal way to bind input to OrtValue.
            This way you retain control over the original value, can modify the data
            using OrtValue interfaces between the runs.
            
            You can also create OrtValue on all kinds of memory, managed, native, stack and device(gpu).
            </summary>
            <param name="name">input name</param>
            <param name="ortValue"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInput(System.String,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],Microsoft.ML.OnnxRuntime.OrtMemoryAllocation)">
            <summary>
            Bind a piece of pre-allocated native memory as a OrtValue Tensor with a given shape
            to an input with a given name. The model will read the specified input from that memory
            possibly avoiding the need to copy between devices. OrtMemoryAllocation continues to own
            the chunk of native memory, and the allocation should be alive until the end of execution.
            </summary>
            <param name="name">of the input</param>
            <param name="elementType">Tensor element type</param>
            <param name="shape"></param>
            <param name="allocation">native memory allocation</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInput(System.String,Microsoft.ML.OnnxRuntime.OrtExternalAllocation)">
            <summary>
            Bind externally (not from OrtAllocator) allocated memory as input.
            The model will read the specified input from that memory
            possibly avoiding the need to copy between devices. The user code continues to own
            the chunk of externally allocated memory, and the allocation should be alive until the end of execution.
            </summary>
            <param name="name">name</param>
            <param name="allocation">non ort allocated memory</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInput(System.String,Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue)">
            <summary>
            Bind the input with the given name as an OrtValue Tensor allocated in pinned managed memory.
            Instance of FixedBufferOnnxValue owns the memory and should be alive until the end of execution.
            </summary>
            <param name="name">name of input</param>
            <param name="fixedValue"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.SynchronizeBoundInputs">
            <summary>
            Blocks until device completes all preceding requested tasks.
            Useful for memory synchronization.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutput(System.String,Microsoft.ML.OnnxRuntime.OrtValue)">
            <summary>
            This is the preferable and universal way to bind output via OrtValue.
            This way you retain control over the original value, can modify the data
            using OrtValue interfaces between the runs, rebind output to input if you
            are feeding data circular.
            </summary>
            <param name="name">output name</param>
            <param name="ortValue">OrtValue to bind</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutput(System.String,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],Microsoft.ML.OnnxRuntime.OrtMemoryAllocation)">
            <summary>
            Bind model output to an OrtValue as Tensor with a given type and shape. An instance of OrtMemoryAllocaiton
            owns the memory and should be alive for the time of execution.
            </summary>
            <param name="name">of the output</param>
            <param name="elementType">tensor element type</param>
            <param name="shape">tensor shape</param>
            <param name="allocation">allocated memory</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutput(System.String,Microsoft.ML.OnnxRuntime.OrtExternalAllocation)">
            <summary>
            Bind externally (not from OrtAllocator) allocated memory as output.
            The model will read the specified input from that memory
            possibly avoiding the need to copy between devices. The user code continues to own
            the chunk of externally allocated memory, and the allocation should be alive until the end of execution.
            </summary>
            <param name="name">name</param>
            <param name="allocation">non ort allocated memory</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutput(System.String,Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue)">
            <summary>
            Bind model output to a given instance of FixedBufferOnnxValue which owns the underlying
            pinned managed memory and should be alive for the time of execution.
            </summary>
            <param name="name">of the output</param>
            <param name="fixedValue"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutputToDevice(System.String,Microsoft.ML.OnnxRuntime.OrtMemoryInfo)">
            <summary>
            This function will bind model output with the given name to a device
            specified by the memInfo.
            </summary>
            <param name="name">output name</param>
            <param name="memInfo">instance of memory info</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.SynchronizeBoundOutputs">
            <summary>
            Blocks until device completes all preceding requested tasks.
            Useful for memory synchronization.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOrtAllocation(System.String,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],Microsoft.ML.OnnxRuntime.OrtMemoryAllocation,System.Boolean)">
            <summary>
            Bind allocation obtained from an Ort allocator
            </summary>
            <param name="name">name </param>
            <param name="elementType">data type</param>
            <param name="shape">tensor shape</param>
            <param name="allocation">ort allocation</param>
            <param name="isInput">whether this is input or output</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindExternalAllocation(System.String,Microsoft.ML.OnnxRuntime.OrtExternalAllocation,System.Boolean)">
            <summary>
            Bind external allocation as input or output.
            The allocation is owned by the user code.
            </summary>
            <param name="name">name </param>
            <param name="allocation">non ort allocated memory</param>
            <param name="isInput">whether this is an input or output</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInputOrOutput(System.String,System.IntPtr,System.Boolean)">
            <summary>
            Internal helper
            </summary>
            <param name="name"></param>
            <param name="ortValue"></param>
            <param name="isInput"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.GetOutputNames">
            <summary>
            Returns an array of output names in the same order they were bound
            </summary>
            <returns>array of output names</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.OrtIoBinding.GetOutputValues" -->
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.ClearBoundInputs">
            <summary>
            Clear all bound inputs and start anew
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.ClearBoundOutputs">
            <summary>
            Clear all bound outputs
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtIoBidning
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtLoggingLevel">
            <summary>
            Log severity levels
            
            Must in sync with OrtLoggingLevel in onnxruntime_c_api.h
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtThreadingOptions">
            <summary>
            This class allows to specify global thread pool options
            when instantiating the ONNX Runtime environment for the first time.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.Handle">
            <summary>
            A pointer to a underlying native instance of ThreadingOptions
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.#ctor">
            <summary>
            Create an empty threading options.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.GlobalInterOpNumThreads">
            <summary>
            Set global inter-op thread count.
            Setting it to 0 will allow ORT to choose the number of threads used for parallelization of
            multiple kernels. Setting it to 1 will cause the main thread to be used (i.e., no thread pools will be used).
            This setting is only meaningful when the execution mode is set to Parallel.
            </summary>
            <param name="value">The number of threads.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.GlobalIntraOpNumThreads">
            <summary>
            Sets the number of threads available for intra-op parallelism (i.e. within a single op).
            Setting it to 0 will allow ORT to choose the number of threads, setting it to 1 will cause the main thread to be used (i.e., no thread pools will be used).
            </summary>
            <param name="value">The number of threads.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.GlobalSpinControl">
            <summary>
            Allows spinning of thread pools when their queues are empty in anticipation of imminent task arrival.
            This call sets the value for both inter-op and intra-op thread pools.
            If the CPU usage is very high then do not enable this.
            </summary>
            <param name="value">If true allow the thread pools to spin.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.SetGlobalDenormalAsZero">
            <summary>
            When this is set it causes intra-op and inter-op thread pools to flush denormal values to zero.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of ThreadingOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtTypeInfo">
            <summary>
            This class retrieves Type Information for input/outputs of the model.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTypeInfo.OnnxType">
            <summary>
            Represents OnnxValueType of the OrtTypeInfo
            </summary>
            <value>OnnxValueType</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTypeInfo.TensorTypeAndShapeInfo">
            <summary>
            This property returns the tensor type and shape information from the OrtTypeInfo
            iff this OrtTypeInfo represents a tensor.
            </summary>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
            <value>Instance of OrtTensorTypeAndShapeInfo</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTypeInfo.SequenceTypeInfo">
            <summary>
            Sequence type information from the OrtTypeInfo iff this OrtTypeInfo represents a sequence.
            </summary>
            <returns>Instance of OrtSequenceTypeInfo</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTypeInfo.MapTypeInfo">
            <summary>
            Represents MapTypeInfo from the OrtTypeInfo iff this OrtTypeInfo represents a map.
            </summary>
            <value>Instance of MapTypeInfo</value>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTypeInfo.OptionalTypeInfo">
            <summary>
            Fetches OptionalTypeInfo from the OrtTypeInfo iff this OrtTypeInfo represents a optional type.
            </summary>
            <returns>Instance of OptionalTypeInfo</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtTensorTypeAndShapeInfo">
            <summary>
            This struct represents type and shape information for a tensor.
            It may describe a tensor type that is a model input or output or
            an information that can be extracted from a tensor in OrtValue.
            
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTensorTypeAndShapeInfo.ElementDataType">
            <summary>
            Fetches tensor element data type
            </summary>
            <value>enum value for the data type</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTensorTypeAndShapeInfo.IsString">
            <summary>
            Returns true if this data element is a string
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTensorTypeAndShapeInfo.ElementCount">
            <summary>
            Fetches tensor element count based on the shape information.
            </summary>
            <returns>number of typed tensor elements</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTensorTypeAndShapeInfo.DimensionsCount">
            <summary>
            Fetches shape dimension count (rank) for the tensor.
            </summary>
            <returns>dim count</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTensorTypeAndShapeInfo.Shape">
            <summary>
            Tensor dimensions.
            </summary>
            <value>array of dims</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtSequenceOrOptionalTypeInfo">
            <summary>
            Represents Sequence type information.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtSequenceOrOptionalTypeInfo.ElementType">
            <summary>
            Returns type information for the element type of the sequence
            </summary>
            <value>OrtTypeInfo</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtMapTypeInfo">
            <summary>
            Represents Map input/output information.
            
            Maps are represented at run time by a tensor of primitive types
            and values are represented either by Tensor/Sequence/Optional or another map.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMapTypeInfo.KeyType">
            <summary>
            Returns KeyType which is the data type of the keys tensor
            </summary>
            <value>OrtTypeInfo</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMapTypeInfo.ValueType">
            <summary>
            Returns an instance of OrtTypeInfo describing the value type for the map
            </summary>
            <value>OrtTypeInfo</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OnnxValueType">
            <summary>
            A type of data that OrtValue encapsulates.
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtValue">
            <summary>
            Represents a disposable OrtValue.
            This class exposes a native instance of OrtValue.
            The class implements IDisposable and must
            be disposed of, otherwise native resources will leak
            and will eventually cause the application to slow down or crash.
            
            If the OrtValue instance is constructed over a managed memory, and it is not
            disposed properly, the pinned memory will continue to be pinned and interfere
            with GC operation.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.#ctor(System.IntPtr,Microsoft.ML.OnnxRuntime.OnnxValueType)">
            <summary>
            Constructor. The newly constructed OrtValue takes ownership of the native OrtValue instance
            </summary>
            <param name="handle"></param>
            <param name="onnxValueType"></param>
            <exception cref="T:System.ArgumentException">thrown when onnxValue type is not known</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.#ctor(System.IntPtr,Microsoft.ML.OnnxRuntime.OnnxValueType,Microsoft.ML.OnnxRuntime.DisposableList{Microsoft.ML.OnnxRuntime.OrtValue}@)">
            <summary>
            Constructor. The newly constructed OrtValue takes ownership of the native OrtValue instance
            and disposes of it when the OrtValue instance is disposed. The instance will take ownership and will
            dispose of compositeMembers instances.
            
            This constructor can only throw if OnnxType is not specified.
            </summary>
            <param name="handle">native ortValue handle</param>
            <param name="onnxValueType">must one of the valid types</param>
            <param name="compositeMembers">For composite types this contains dependent ortValues such as members of a sequence
            or keys/values for the map, that may have been created on top of the managed memory and must be disposed
            with the new ortValue. This container will be taken the ownership of and the argument will be set to null.</param>
            <exception cref="T:System.ArgumentException">throws when onnxValueType is not specified</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.#ctor(System.IntPtr,System.Buffers.MemoryHandle)">
            <summary>
            Constructor to construct OrtValue over managed memory.
            We pin the memory and unpin it at the disposal time.
            The newly constructed OrtValue takes ownership of the native OrtValue instance
            and disposes of it when the OrtValue instance is disposed.
            </summary>
            <param name="handle">Pointer to a native instance of OrtValue</param>
            <param name="memHandle">memory handle to a pinned user supplied (usually managed) memory
            It is disposed of (unpinned) when OrtValue is disposed.
            </param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtValue.Handle">
            <summary>
            Native handle to OrtValue for internal use.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtValue.Value">
            <summary>
            Implement IOrtValueOwner interface
            </summary>
            <value>returns this</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtValue.OnnxType">
            <summary>
            Fetches OrtValue type if it has one.
            </summary>
            <value>OnnxValueType</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtValue.IsTensor">
            <summary>
            Returns true if OrtValue contains a tensor
            </summary>
            <returns>true if tensor</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtValue.IsSparseTensor">
            <summary>
            Returns true if OrtValue contains a sparse tensor
            </summary>
            <returns>true if sparse tensor</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetValueCount">
            <summary>
            Valid for composite ML types like map, sequence.
            Returns 2 for map (keys, values) and N for sequence, where N is the number of elements
            in the sequence.
            </summary>
            <returns>Element count</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetValue(System.Int32,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            For non tensors return OrtValue element at the specified index.
            For maps only indices 0 and 1 are valid. For sequences, [0..N) are valid.
            See GetValueCount() to determine the valid range.
            </summary>
            <param name="index"></param>
            <param name="allocator">allocator to use</param>
            <returns>OrtValue disposable instance that points to the corresponding element of the composite type</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetTensorDataAsSpan``1">
            <summary>
            Returns a ReadOnlySpan<typeparamref name="T"/> over tensor native buffer that
            provides a read-only view.
            
            Note, that the memory may be device allocated and, therefore, not accessible from the CPU.
            To get memory descriptor use GetTensorMemoryInfo().
            
            OrtValue must contain a non-string tensor.
            The span is valid as long as the OrtValue instance is alive (not disposed).
            </summary>
            <typeparam name="T"></typeparam>
            <returns>ReadOnlySpan<typeparamref name="T"/></returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetTensorMutableDataAsSpan``1">
            <summary>
            Returns a Span<typeparamref name="T"/> over tensor native buffer.
            This enables you to safely and efficiently modify the underlying
            native buffer in a type-safe manner. This is useful for example in IOBinding scenarios
            where you want to modify results of the inference and feed it back as input.
            
            Note, that the memory may be device allocated.
            To get memory descriptor use GetTensorMemoryInfo().
            
            OrtValue must contain a non-string tensor.
            The span is valid as long as the OrtValue instance is alive (not disposed).
            </summary>
            <typeparam name="T"></typeparam>
            <returns>Typed Span over the native buffer</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetTensorMutableRawData">
            <summary>
            Provides mutable raw native buffer access.
            </summary>
            <returns>Span over the native buffer bytes</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.OrtValue.GetStringElementAsMemory(System.Int32)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetStringElement(System.Int32)">
            <summary>
            Fetch string tensor element buffer pointer at the specified index,
            copy/convert UTF-8 into a UTF-16 string and return it.
            
            Obtain TensorTypeAndShape to get shape and element count.
            </summary>
            <param name="index">flat string tensor element index</param>
            <returns>UTF-16 string instance</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetStringElementAsSpan(System.Int32)">
            <summary>
            Get a span over the native memory of the string tensor element.
            The span is valid as long as the OrtValue is valid.
            
            This is useful if you want to perform your own UTF-8 decoding or
            you do not care about decoding.
            Obtain TensorTypeAndShape to get shape and element count.
            </summary>
            <param name="index">flat element index</param>
            <returns>ReadOnlySpan over UTF-8 bytes of the string tensor element</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetStringTensorAsArray">
            <summary>
            Convenience method to obtain all string tensor elements as a string array.
            </summary>
            <returns>string[]</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetTypeInfo">
            <summary>
            Creates and fetches Type information about the contained OnnxValue.
            </summary>
            <returns>a disposable instance of OrtTypeInfo</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetTensorTypeAndShape">
            <summary>
            Obtains Tensor And Type Information from the OrtValue iff it contains a tensor.
            Valid only for OrtValues that contain a tensor.
            </summary>
            <returns>A disposable instance of OrtTensorTypeAndShapeInfo</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.GetTensorMemoryInfo">
            <summary>
            Returns OrtMemoryInfo iff this OrtValue contains a tensor or a sparse tensor.
            </summary>
            <returns>OrtMemoryInfo that describes the underlying memory allocation</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateTensorValueWithData(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],System.IntPtr,System.Int64)">
            <summary>
            Factory method to construct an OrtValue of Tensor type on top of pre-allocated memory.
            This can be a piece of arbitrary memory that may be allocated by OrtAllocator (possibly on a device),
            a chunk of managed memory (must be pinned for the duration of OrtValue lifetime) or a memory that is allocated
            natively allocated using Marshal.AllocHGlobal(), stackalloc or other means (may be on a device).
            
            The resulting OrtValue does not own the underlying memory buffer and will not attempt to
            deallocate it. The caller must make sure that the memory remains valid for the duration of OrtValue lifetime.
            </summary>
            <param name="memInfo">Memory Info. For managed memory its default is cpu.
                                  For other kinds of memory, one must construct as appropriate.</param>
            <param name="elementType">DataType for the Tensor</param>
            <param name="shape">shape of the tensor to create. The size required by the shape
            must be less of equal of the memory.Length</param>
            <param name="dataBufferPtr">Pointer to a raw memory buffer which may reside on a device</param>
            <param name="bufferLengthInBytes">Buffer length in bytes</param>
            <returns>A disposable instance of OrtValue</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateTensorValueFromMemory``1(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,System.Memory{``0},System.Int64[])">
            <summary>
            This is a factory method that creates an OrtValue of Tensor type on top of Memory<typeparamref name="T"/> memory.
            The API pins the memory for the duration of the OrtValue lifetime.
            It is unpinned at disposal time.
            </summary>
            <typeparam name="T">T must be one of the supported types</typeparam>
            <param name="memoryInfo">Memory information that describes memory location</param>
            <param name="memory">contiguous region of memory</param>
            <param name="shape">shape of the tensor to create. The size required by the shape
            must be less of equal of the memory.Length</param>
            <returns>A disposable OrtValue instance</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateTensorValueFromMemory``1(``0[],System.Int64[])">
            <summary>
            This is a factory method that creates an OrtValue of Tensor type on top managed data array.
            The API pins the memory for the duration of the OrtValue lifetime.
            It is unpinned at disposal time.
            </summary>
            <typeparam name="T"></typeparam>
            <param name="data">managed data buffer</param>
            <param name="shape">shape that describes the buffer</param>
            <returns>A disposable OrtValue instance</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateAllocatedTensorValue(Microsoft.ML.OnnxRuntime.OrtAllocator,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[])">
            <summary>
            The factory API creates an OrtValue with memory allocated using the given allocator
            according to the specified shape and element type. The memory will be released when OrtValue
            is disposed. Use GetTensorMutableDataAsSpan&lt;T&gt;() API to fill in the data.
            </summary>
            <param name="allocator"></param>
            <param name="elementType"></param>
            <param name="shape"></param>
            <returns>A disposable OrtValue</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateFromTensorObject(Microsoft.ML.OnnxRuntime.Tensors.TensorBase,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType@)">
            <summary>
            This is a factory method creates a native Onnxruntime OrtValue containing a tensor.
            The method will attempt to pin managed memory so no copying occurs when data is passed down
            to native code.
            </summary>
            <param name="value">Tensor object</param>
            <param name="elementType">discovered tensor element type</param>
            <returns>And instance of OrtValue constructed on top of the object</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateTensorWithEmptyStrings(Microsoft.ML.OnnxRuntime.OrtAllocator,System.Int64[])">
            <summary>
            Creates an OrtValue that contains a string tensor of specified shape, and
            containing empty strings. String tensors are always on CPU.
            Use StringTensorSetElementAt to assign individual elements values.
            </summary>
            <param name="allocator"></param>
            <returns>disposable OrtValue</returns>
            <param name="shape">tensor shape</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.StringTensorSetElementAt(System.ReadOnlySpan{System.Char},System.Int32)">
            <summary>
            Converts the string argument represented by ReadOnlySpan to UTF-8,
            allocates space in the native tensor and copies it into the native tensor memory.
            Typically, this is used to populate a new empty string tensor element.
            
            The number of elements is according to the shape supplied to CreateTensorWithEmptyStrings().
            However, this API can also be used to overwrite any existing element within the string tensor.
            
            In general, to obtain the number of elements for any tensor, use GetTensorTypeAndShape() which
            would return a disposable instance of TensorTypeAndShapeInfo. 
            Then call GetElementCount() or GetShape().
            </summary>
            <param name="str">ReadOnlySpan over chars</param>
            <param name="index">index of the string element within the tensor
            must be within bounds of [0, N)</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.StringTensorSetElementAt(System.ReadOnlyMemory{System.Char},System.Int32)">
             <summary>
             Converts the string argument represented by ReadOnlyMemory to UTF-8,
             allocates space in the native tensor and copies it into the native tensor memory.
             Typically, this is used to populate a new empty string tensor element.
             
             The number of elements is according to the shape supplied to CreateTensorWithEmptyStrings().
             However, this API can also be used to overwrite any existing element within the string tensor.
             
             In general, to obtain the number of elements for any tensor, use GetTensorTypeAndShape() which
             would return a disposable instance of TensorTypeAndShapeInfo. 
             Then call GetElementCount() or GetShape().
            
             </summary>
             <param name="rom">ReadOnlyMemory instance over an array of chars</param>
             <param name="index">index of the string element within the tensor
             must be within bounds of [0, N)</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.StringTensorSetElementAt(System.ReadOnlySpan{System.Byte},System.Int32)">
            <summary>
            This API resizes String Tensor element to the requested amount of bytes (UTF-8)
            and copies the bytes from the supplied ReadOnlySpan into the native tensor memory (resized buffer).
            
            The API is useful for quick loading of utf8 data into the native tensor memory.
            </summary>
            <param name="utf8Bytes">read only span of bytes</param>
            <param name="index">flat index of the element in the string tensor</param>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.OrtValue.CreateFromStringTensor(Microsoft.ML.OnnxRuntime.Tensors.Tensor{System.String})" -->
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateSequence(System.Collections.Generic.ICollection{Microsoft.ML.OnnxRuntime.OrtValue})">
            <summary>
            Creates a sequence of OrtValues from a collection of OrtValues.
            All OrtValues in the collection must be of the same Onnx type.
            I.e. (Tensor, SparseTensor, Map, Sequence, etc.)
            
            The ortValues that are passed as argument are taken possession of by the newly
            created OrtValue. The caller should not dispose them, unless this call fails.
            
            The ortValues would be empty on successful return.
            </summary>
            <param name="ortValues">a collection of OrtValues. On success the ortValues contained in the list
            are taken ownership of and the list is cleared.</param>
            <returns>A disposable instance of OrtValues</returns>
            <exception cref="T:System.ArgumentNullException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateSequence(Microsoft.ML.OnnxRuntime.DisposableList{Microsoft.ML.OnnxRuntime.OrtValue}@)">
            <summary>
            Creates a sequence from the values in compositeMembers
            The argument is taken possession of and is nullified on successful return.
            </summary>
            <param name="compositeMembers">sequence ortValues</param>
            <returns>OrtValue instance representing a Sequence</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtValue.SequenceElementVisitor">
            <summary>
            A delegate type that is expected to process each OrtValue in a sequence.
            </summary>
            <param name="ortValue">OrtValue that holds sequence element</param>
            <param name="index">ordinal of the value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.ProcessSequence(Microsoft.ML.OnnxRuntime.OrtValue.SequenceElementVisitor,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            Feeds each OrtValue in a sequence to the visitor delegate.
            This helps users to avoid dealing each value life-span
            </summary>
            <param name="visitor">visitor delegate</param>
            <param name="allocator">allocator to use for intermediate ort values</param>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateMap(Microsoft.ML.OnnxRuntime.OrtValue@,Microsoft.ML.OnnxRuntime.OrtValue@)">
            <summary>
            Creates a map OrtValue with keys and values.
            On a high level the Onnxruntime representation of the map always consists of two
            OrtValues, keys and values.
            
            According to ONNX standard map keys can be unmanaged types only (or strings).
            Those keys are contained in a single tensor within OrtValue keys.
            
            Map values, on the other hand, can be composite types. The values parameter
            can either contain a single tensor with unmanaged map values with the same number of
            elements as the keys, or it can be a sequence of OrtValues,
            each of those can be a composite type (tensor, sequence, map). If it is a sequence,
            then the number of elements must match the number of elements in keys.
            
            Keys and values must be in the same order.
            
            ORT supports only a subset of types for keys and values, however, this API does not
            restrict it.
            
            The ortValues that are passed as argument are taken possession of by the newly
            created OrtValue. The caller should not dispose them, unless this call fails.
            
            Keys and values arguments will be set to null on success.
            </summary>
            <param name="keys">Contains keys</param>
            <param name="values">Contains values</param>
            <returns>A disposable OrtValue</returns>
            <exception cref="T:System.ArgumentNullException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateMap``2(``0[],``1[])">
            <summary>
            This API helps to quickly creates a map OrtValue with unmanaged (primitive) keys and values specified as arrays.
            This helps the user not to create OrtValues for keys and values separately and deal only with the final result.
            The map would consist of two tensors, one for keys and one for values.
            
            The OrtValues would be created on top of the managed memory arrays and use it directly.
            The number of elements in keys and values must be the same and they must be in order.
            
            The types must be unmanaged.
            </summary>
            <typeparam name="K">keys type</typeparam>
            <typeparam name="V">values type</typeparam>
            <param name="keys">array of keys of K type</param>
            <param name="values">array of values of V type</param>
            <returns>OrtValue instance</returns>
            <exception cref="T:System.ArgumentNullException"></exception>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateMapWithStringKeys``1(System.Collections.Generic.IReadOnlyCollection{System.String},``0[])">
            <summary>
            Creates a map OrtValue with string keys and non-string values.
            This helps the user not to create OrtValues for keys and values separately.
            The number of elements in keys and values must be the same and they must be in order.
            The map would consist of two tensors, one for keys and one for values.
            
            string keys would be converted to UTF-8 encoding and copied to an allocated native memory.
            The OrtValue for values would be created on top of the managed memory using it directly.
            
            The values type must be unmanaged.
            </summary>
            <typeparam name="V"></typeparam>
            <param name="keys">Collection of strings</param>
            <param name="values"></param>
            <returns>OrtValue instance</returns>
            <exception cref="T:System.ArgumentNullException"></exception>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateMapWithStringValues``1(``0[],System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            Creates a map OrtValue with non-string keys and string values.
            
            This helps the user not to create OrtValues for keys and values separately.
            The number of elements in keys and values must be the same and they must be in order.
            
            The OrtValue for keys would be created on top of the managed memory using it directly.
            string values would be converted to UTF-8 encoding and copied to an allocated native memory.
            
            </summary>
            <typeparam name="K">unmanaged type of keys</typeparam>
            <param name="keys"></param>
            <param name="values">collection of string values</param>
            <returns>Instance of OrtValue</returns>
            <exception cref="T:System.ArgumentNullException"></exception>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtValue.MapVisitor">
            <summary>
            A public delegate that will be invoked once with map keys and values.
            The delegate helps not to deal with the lifespan of intermediate OrtValues.
            Typically, when one uses GetValue() API, it creates a copy of OrtValue
            that points to the same buffer as keys or values. This API helps to deal with those
            temporary instances and avoid leaks.
            
            According to ONNX standard map keys can be unmanaged types only (or strings).
            Those keys are contained in a single tensor within OrtValue keys. So you can query those
            directly from keys argument.
            
            Map values, on the other hand, can be composite types. The values parameter
            can either contain a single tensor with unmanaged map values with the same number of
            elements as the keys, or it can be a sequence of OrtValues,
            each of those can be a composite type (tensor, sequence, map). If it is a sequence,
            then the number of elements must match the number of elements in keys.
            
            Depending on the structure of the values, one will either directly query a single tensor
            from values, or will have to iterate over the sequence of OrtValues and visit each of those
            resulting in a recursive visitation.
            </summary>
            <param name="keys">This would always represent a tensor</param>
            <param name="values">Can be any of the Onnx types, but they would all reduce to tensors eventually</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.ProcessMap(Microsoft.ML.OnnxRuntime.OrtValue.MapVisitor,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            This API helps the user to process a map OrtValue without
            having to deal with the lifespan of intermediate OrtValues.
            
            each API value is fed to the vistor functor.
            </summary>
            <param name="visitor">visitor function</param>
            <param name="allocator">Allocator to use for intermediate values</param>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked from Dispose() method</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.IOrtValueOwner">
            <summary>
            Provides access from the underlying object that owns disposable OrtValue
            The returned value does not own the actual memory and does nothing on Dispose()
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeOrtValueCollectionOwner">
            <summary>
            This class is used in conjunction with DisposableNamedOnnxValue to 
            own native collection OrtValue and dispose of it along with any DisposableNamedOnnxValues
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOrtValueCollectionOwner.#ctor(Microsoft.ML.OnnxRuntime.OrtValue@,System.IDisposable)">
            <summary>
            _Ctor. Takes ownership of ortValue and sets it to null on success.
            </summary>
            <param name="ortValue">becomes null on success</param>
            <param name="disposables">A collection of disposables that support composed types.
            We stick them here and dispose them when this instance is disposed.
            </param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NativeOrtValueCollectionOwner.Value">
            <summary>
            Returns OrtValue that is owned by this instance
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.OrtValueTensor`1" -->
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValueTensor`1.#ctor(Microsoft.ML.OnnxRuntime.OrtValue@)">
            <summary>
            Constructs an instance and takes ownership of ortValue on success
            </summary>
            <param name="ortValue">ortValue that is a Tensor. It becomes null on successful return.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtValueTensor`1.Value">
            <summary>
            Returns OrtValue that is owned by this instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValueTensor`1.GetSpan">
            <summary>
            Returns Span that is a view into the underlying native Tensor memory
            </summary>
            <returns>SpanT</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValueTensor`1.Pin(System.Int32)">
            <summary>
            Satisfy MemoryManager abstract implementation.
            </summary>
            <param name="elementIndex">required for override</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer">
            <summary>
            This class holds pre-packed weights of shared initializers to be shared across sessions using these initializers
            and thereby provide memory savings by sharing the same pre-packed versions of these shared initializers
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer.#ctor">
            <summary>
            Constructs an empty PrePackedWeightsContainer
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer.Pointer">
            <summary>
            Internal accessor to call native methods
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to deallocate
            a chunk of memory using the specified allocator.
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions">
            <summary>
            Holds the options for configuring a TensorRT Execution Provider instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.#ctor">
            <summary>
            Constructs an empty OrtTensorRTProviderOptions instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.GetOptions">
            <summary>
            Get TensorRT EP provider options
            </summary>
            <returns> return C# UTF-16 encoded string </returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.UpdateOptions(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Updates  the configuration knobs of OrtTensorRTProviderOptions that will eventually be used to configure a TensorRT EP
            Please refer to the following on different key/value pairs to configure a TensorRT EP and their meaning:
            https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html
            </summary>
            <param name="providerOptions">key/value pairs used to configure a TensorRT Execution Provider</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.GetDeviceId">
            <summary>
            Get device id of TensorRT EP.
            </summary>
            <returns> device id </returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtTensorRTProviderOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions">
            <summary>
            Holds the options for configuring a CUDA Execution Provider instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.#ctor">
            <summary>
            Constructs an empty OrtCUDAroviderOptions instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.GetOptions">
            <summary>
            Get CUDA EP provider options
            </summary>
            <returns> return C# UTF-16 encoded string </returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.UpdateOptions(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Updates  the configuration knobs of OrtCUDAProviderOptions that will eventually be used to configure a CUDA EP
            Please refer to the following on different key/value pairs to configure a CUDA EP and their meaning:
            https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html
            </summary>
            <param name="providerOptions">key/value pairs used to configure a CUDA Execution Provider</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtCUDAProviderOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtROCMProviderOptions">
            <summary>
            Holds the options for configuring a ROCm Execution Provider instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtROCMProviderOptions.#ctor">
            <summary>
            Constructs an empty OrtROCMroviderOptions instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtROCMProviderOptions.GetOptions">
            <summary>
            Get ROCm EP provider options
            </summary>
            <returns> return C# UTF-16 encoded string </returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtROCMProviderOptions.UpdateOptions(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Updates  the configuration knobs of OrtROCMProviderOptions that will eventually be used to configure a ROCm EP
            Please refer to the following on different key/value pairs to configure a ROCm EP and their meaning:
            https://onnxruntime.ai/docs/execution-providers/ROCm-ExecutionProvider.html
            </summary>
            <param name="providerOptions">key/value pairs used to configure a ROCm Execution Provider</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtROCMProviderOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtROCMProviderOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtROCMProviderOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ProviderOptionsValueHelper">
            <summary>
            This helper class contains methods to handle values of provider options
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ProviderOptionsValueHelper.StringToDict(System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Parse from string and save to dictionary
            </summary>
            <param name="s">C# string</param>
            <param name="dict">Dictionary instance to store the parsing result of s</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.CoreMLFlags">
            <summary>
            CoreML flags for use with SessionOptions
            </summary>
            <see cref="!:https://github.com/microsoft/onnxruntime/blob/main/include/onnxruntime/core/providers/coreml/coreml_provider_factory.h"/>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NnapiFlags">
            <summary>
            NNAPI flags for use with SessionOptions
            </summary>
            <see cref="!:https://github.com/microsoft/onnxruntime/blob/main/include/onnxruntime/core/providers/nnapi/nnapi_provider_factory.h"/>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.RunOptions">
            <summary>
             Sets various runtime options. 
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.RunOptions.#ctor">
            <summary>
            Default __ctor. Creates default RuntimeOptions
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.LogSeverityLevel">
            <summary>
            Log Severity Level for the session logs. Default = ORT_LOGGING_LEVEL_WARNING
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.LogVerbosityLevel">
            <summary>
            Log Verbosity Level for the session logs. Default = 0. Valid values are >=0.
            This takes into effect only when the LogSeverityLevel is set to ORT_LOGGING_LEVEL_VERBOSE.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.LogId">
            <summary>
            Log tag to be used during the run. default = ""
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.Terminate">
            <summary>
            Sets a flag to terminate all Run() calls that are currently using this RunOptions object 
            Default = false
            </summary>
            <value>terminate flag value</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.RunOptions.AddRunConfigEntry(System.String,System.String)">
            <summary>
            Set a single run configuration entry as a pair of strings
            If a configuration with same key exists, this will overwrite the configuration with the given configValue.
            </summary>
            <param name="configKey">config key name</param>
            <param name="configValue">config key value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.RunOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of RunOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.GraphOptimizationLevel">
            <summary>
            Graph optimization level to use with SessionOptions
             [https://github.com/microsoft/onnxruntime/blob/main/docs/ONNX_Runtime_Graph_Optimizations.md]
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ExecutionMode">
            <summary>
            Controls whether you want to execute operators in the graph sequentially or in parallel.
            Usually when the model has many branches, setting this option to ExecutionMode.ORT_PARALLEL
            will give you better performance.
            See [ONNX_Runtime_Perf_Tuning.md] for more details.
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.SessionOptions">
            <summary>
            Holds the options for creating an InferenceSession
            It forces the instantiation of the OrtEnv singleton.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.#ctor">
            <summary>
            Constructs an empty SessionOptions
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithCudaProvider(System.Int32)">
            <summary>
            A helper method to construct a SessionOptions object for CUDA execution.
            Use only if CUDA is installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId"></param>
            <returns>A SessionsOptions() object configured for execution on deviceId</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithCudaProvider(Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions)">
            <summary>
            A helper method to construct a SessionOptions object for CUDA execution provider.
            Use only if CUDA is installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="cudaProviderOptions">CUDA EP provider options</param>
            <returns>A SessionsOptions() object configured for execution on provider options</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithTensorrtProvider(System.Int32)">
            <summary>
            A helper method to construct a SessionOptions object for TensorRT execution.
            Use only if CUDA/TensorRT are installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId"></param>
            <returns>A SessionsOptions() object configured for execution on deviceId</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithTensorrtProvider(Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions)">
            <summary>
            A helper method to construct a SessionOptions object for TensorRT execution provider.
            Use only if CUDA/TensorRT are installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="trtProviderOptions">TensorRT EP provider options</param>
            <returns>A SessionsOptions() object configured for execution on provider options</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithTvmProvider(System.String)">
            <summary>
            A helper method to construct a SessionOptions object for TVM execution.
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="settings">settings string, comprises of comma separated key:value pairs. default is empty</param>
            <returns>A SessionsOptions() object configured for execution with TVM</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithRocmProvider(System.Int32)">
            <summary>
            A helper method to construct a SessionOptions object for ROCM execution.
            Use only if ROCM is installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">Device Id</param>
            <returns>A SessionsOptions() object configured for execution on deviceId</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithRocmProvider(Microsoft.ML.OnnxRuntime.OrtROCMProviderOptions)">
            <summary>
            A helper method to construct a SessionOptions object for ROCm execution provider.
            Use only if ROCm is installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="rocmProviderOptions">ROCm EP provider options</param>
            <returns>A SessionsOptions() object configured for execution on provider options</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_CPU(System.Int32)">
            <summary>
            Appends CPU EP to a list of available execution providers for the session.
            </summary>
            <param name="useArena">1 - use arena, 0 - do not use arena</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Dnnl(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="useArena">1 - use allocation arena, 0 - otherwise</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_CUDA(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">integer device ID</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_CUDA(Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions)">
            <summary>
            Append a CUDA EP instance (based on specified configuration) to the SessionOptions instance.
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="cudaProviderOptions">CUDA EP provider options</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_DML(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">device identification</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_OpenVINO(System.String)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">device identification, default empty string</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Tensorrt(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">device identification</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Tensorrt(Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions)">
            <summary>
            Append a TensorRT EP instance (based on specified configuration) to the SessionOptions instance.
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="trtProviderOptions">TensorRT EP provider options</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_ROCm(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">Device Id</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_ROCm(Microsoft.ML.OnnxRuntime.OrtROCMProviderOptions)">
            <summary>
            Append a ROCm EP instance (based on specified configuration) to the SessionOptions instance.
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="rocmProviderOptions">ROCm EP provider options</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_MIGraphX(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">device identification</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Nnapi(Microsoft.ML.OnnxRuntime.NnapiFlags)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="nnapiFlags">NNAPI specific flag mask</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_CoreML(Microsoft.ML.OnnxRuntime.CoreMLFlags)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="coremlFlags">CoreML specific flags</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Tvm(System.String)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="settings">string with TVM specific settings</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider(System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Append QNN, SNPE or XNNPACK execution provider
            </summary>
            <param name="providerName">Execution provider to add. 'QNN', 'SNPE' or 'XNNPACK' are currently supported.</param>
            <param name="providerOptions">Optional key/value pairs to specify execution provider options.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.RegisterCustomOpLibrary(System.String)">
             <summary>
             Loads a DLL named 'libraryPath' and looks for this entry point:
               OrtStatus* RegisterCustomOps(OrtSessionOptions* options, const OrtApiBase* api);
             It then passes in the provided session options to this function along with the api base.
            
             Prior to v1.15 this leaked the library handle and RegisterCustomOpLibraryV2
             was added to resolve that.
            
             From v1.15 on ONNX Runtime will manage the lifetime of the handle.
             </summary>
             <param name="libraryPath">path to the custom op library</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.RegisterCustomOpLibraryV2(System.String,System.IntPtr@)">
            <summary>
            Loads a DLL named 'libraryPath' and looks for this entry point:
            OrtStatus* RegisterCustomOps(OrtSessionOptions* options, const OrtApiBase* api);
            It then passes in the provided session options to this function along with the api base.
            The handle to the loaded library is returned in 'libraryHandle'.
            It can be unloaded by the caller after all sessions using the passed in
            session options are destroyed, or if an error occurs and it is non null.
            Hint: .NET Core 3.1 has a 'NativeLibrary' class that can be used to free the library handle
            </summary>
            <param name="libraryPath">Custom op library path</param>
            <param name="libraryHandle">out parameter, library handle</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.RegisterOrtExtensions">
            <summary>
            Register the custom operators from the Microsoft.ML.OnnxRuntime.Extensions NuGet package.
            A reference to Microsoft.ML.OnnxRuntime.Extensions must be manually added to your project.
            </summary>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException">Throws if the extensions library is not found.</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AddInitializer(System.String,Microsoft.ML.OnnxRuntime.OrtValue)">
            <summary>
            Add a pre-allocated initializer to a session. If a model contains an initializer with a name
            that is same as the name passed to this API call, ORT will use this initializer instance
            instead of deserializing one from the model file. This is useful when you want to share
            the same initializer across sessions.
            </summary>
            <param name="name">name of the initializer</param>
            <param name="ortValue">OrtValue containing the initializer. Lifetime of 'val' and the underlying initializer buffer must be
            managed by the user (created using the CreateTensorWithDataAsOrtValue API) and it must outlive the session object</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AddSessionConfigEntry(System.String,System.String)">
            <summary>
            Set a single session configuration entry as a pair of strings
            If a configuration with same key exists, this will overwrite the configuration with the given configValue
            </summary>
            <param name="configKey">config key name</param>
            <param name="configValue">config key value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AddFreeDimensionOverride(System.String,System.Int64)">
            <summary>
            Override symbolic dimensions (by specific denotation strings) with actual values if known at session initialization time to enable
            optimizations that can take advantage of fixed values (such as memory planning, etc)
            </summary>
            <param name="dimDenotation">denotation name</param>
            <param name="dimValue">denotation value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AddFreeDimensionOverrideByName(System.String,System.Int64)">
            <summary>
            Override symbolic dimensions (by specific name strings) with actual values if known at session initialization time to enable
            optimizations that can take advantage of fixed values (such as memory planning, etc)
            </summary>
            <param name="dimName">dimension name</param>
            <param name="dimValue">dimension value</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.EnableMemoryPattern">
            <summary>
            Enables the use of the memory allocation patterns in the first Run() call for subsequent runs. Default = true.
            </summary>
            <value>returns enableMemoryPattern flag value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.ProfileOutputPathPrefix">
            <summary>
            Path prefix to use for output of profiling data
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.EnableProfiling">
            <summary>
            Enables profiling of InferenceSession.Run() calls. Default is false
            </summary>
            <value>returns _enableProfiling flag value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.OptimizedModelFilePath">
            <summary>
             Set filepath to save optimized model after graph level transformations. Default is empty, which implies saving is disabled.
            </summary>
            <value>returns _optimizedModelFilePath flag value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.EnableCpuMemArena">
            <summary>
            Enables Arena allocator for the CPU memory allocations. Default is true.
            </summary>
            <value>returns _enableCpuMemArena flag value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.LogId">
            <summary>
            Log Id to be used for the session. Default is empty string.
            </summary>
            <value>returns _logId value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.LogSeverityLevel">
            <summary>
            Log Severity Level for the session logs. Default = ORT_LOGGING_LEVEL_WARNING
            </summary>
            <value>returns _logSeverityLevel value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.LogVerbosityLevel">
            <summary>
            Log Verbosity Level for the session logs. Default = 0. Valid values are >=0.
            This takes into effect only when the LogSeverityLevel is set to ORT_LOGGING_LEVEL_VERBOSE.
            </summary>
            <value>returns _logVerbosityLevel value</value>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Microsoft.ML.OnnxRuntime.SessionOptions.IntraOpNumThreads" -->
        <!-- Badly formed XML comment ignored for member "P:Microsoft.ML.OnnxRuntime.SessionOptions.InterOpNumThreads" -->
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.GraphOptimizationLevel">
            <summary>
            Sets the graph optimization level for the session. Default is set to ORT_ENABLE_ALL.
            </summary>
            <value>returns _graphOptimizationLevel value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.ExecutionMode">
            <summary>
            Sets the execution mode for the session. Default is set to ORT_SEQUENTIAL.
            See [ONNX_Runtime_Perf_Tuning.md] for more details.
            </summary>
            <value>returns _executionMode value</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of SessionOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.SessionOptionsContainer">
            <summary>
            Helper to allow the creation/addition of session options based on pre-defined named entries.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.Register(System.Action{Microsoft.ML.OnnxRuntime.SessionOptions})">
            <summary>
            Register the default handler. This is used when a configuration name is not provided.
            </summary>
            <param name="defaultHandler">Handler that applies the default settings to a SessionOptions instance.
            </param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.Register(System.String,System.Action{Microsoft.ML.OnnxRuntime.SessionOptions})">
            <summary>
            Register a named handler.
            </summary>
            <param name="configuration">Configuration name.</param>
            <param name="handler">
            Handler that applies the settings for the configuration to a SessionOptions instance.
            </param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.Create(System.String,System.Boolean)">
            <summary>
            Create a SessionOptions instance with configuration applied.
            </summary>
            <param name="configuration">
            Configuration to use. 
            If not provided, the default set of session options will be applied if useDefaultAsFallback is true.
            </param>
            <param name="useDefaultAsFallback">
            If configuration is not provided or not found, use the default session options.
            </param>
            <returns>SessionOptions with configuration applied.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.Reset">
            <summary>
            Reset by removing all registered handlers.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.ApplyConfiguration(Microsoft.ML.OnnxRuntime.SessionOptions,System.String,System.Boolean)">
            <summary>
            Apply a configuration to a SessionOptions instance.
            </summary>
            <param name="options">SessionOptions to apply configuration to.</param>
            <param name="configuration">Configuration to use.</param>
            <param name="useDefaultAsFallback">
            Use the default configuration if 'configuration' is not provided or not found.
            </param>
            <returns>Updated SessionOptions instance.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.ShapeUtils">
            <summary>
            This class contains utilities for useful calculations with shape.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ShapeUtils.GetSizeForShape(System.ReadOnlySpan{System.Int64})">
            <summary>
            Returns a number of elements in the tensor from the given shape
            </summary>
            <param name="shape"></param>
            <returns>size</returns>
            <exception cref="T:System.ArgumentOutOfRangeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ShapeUtils.GetStrides(System.ReadOnlySpan{System.Int64})">
            <summary>
            Gets the set of strides that can be used to calculate the offset of n-dimensions in a 1-dimensional layout
            </summary>
            <param name="dimensions"></param>
            <returns>an array of strides</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ShapeUtils.GetIndex(System.ReadOnlySpan{System.Int64},System.ReadOnlySpan{System.Int64},System.Int32)">
            <summary>
            Calculates the 1-d index for n-d indices in layout specified by strides.
            </summary>
            <param name="strides">pre-calculated strides</param>
            <param name="indices">Indices. Must have the same length as strides</param>
            <param name="startFromDimension"></param>
            <returns>A 1-d index into the tensor buffer</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions" -->
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(``0[])">
            <summary>
            Creates a copy of this single-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <returns>A 1-dimensional DenseTensor&lt;T&gt; with the same length and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(``0[0:,0:],System.Boolean)">
            <summary>
            Creates a copy of this two-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): row-major.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): column-major.</param>
            <returns>A 2-dimensional DenseTensor&lt;T&gt; with the same dimensions and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(``0[0:,0:,0:],System.Boolean)">
            <summary>
            Creates a copy of this three-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
            <returns>A 3-dimensional DenseTensor&lt;T&gt; with the same dimensions and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(``0[0:,0:,0:,0:],System.Boolean)">
            <summary>
            Creates a copy of this four-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
            <returns>A 4-dimensional DenseTensor&lt;T&gt; with the same dimensions and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(System.Array,System.Boolean)">
            <summary>
            Creates a copy of this n-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
            <returns>A n-dimensional DenseTensor&lt;T&gt; with the same dimensions and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.GetStrides(System.ReadOnlySpan{System.Int32},System.Boolean)">
            <summary>
            Gets the set of strides that can be used to calculate the offset of n-dimensions in a 1-dimensional layout
            </summary>
            <param name="dimensions"></param>
            <param name="reverseStride"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.GetIndex(System.Int32[],System.ReadOnlySpan{System.Int32},System.Int32)">
            <summary>
            Calculates the 1-d index for n-d indices in layout specified by strides.
            </summary>
            <param name="strides"></param>
            <param name="indices"></param>
            <param name="startFromDimension"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.GetIndices(System.ReadOnlySpan{System.Int32},System.Boolean,System.Int32,System.Int32[],System.Int32)">
            <summary>
            Calculates the n-d indices from the 1-d index in a layout specified by strides
            </summary>
            <param name="strides"></param>
            <param name="reverseStride"></param>
            <param name="index"></param>
            <param name="indices"></param>
            <param name="startFromDimension"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.GetIndices(System.ReadOnlySpan{System.Int32},System.Boolean,System.Int32,System.Span{System.Int32},System.Int32)">
            <summary>
            Calculates the n-d indices from the 1-d index in a layout specificed by strides
            </summary>
            <param name="strides"></param>
            <param name="reverseStride"></param>
            <param name="index"></param>
            <param name="indices"></param>
            <param name="startFromDimension"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.TransformIndexByStrides(System.Int32,System.Int32[],System.Boolean,System.Int32[])">
            <summary>
            Takes an 1-d index over n-d sourceStrides and recalculates it assuming same n-d coordinates over a different n-d strides
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1">
            <summary>
            Represents a multi-dimensional collection of objects of type T that can be accessed by indices.  
            DenseTensor stores values in a contiguous sequential block of memory where all values are represented.
            </summary>
            <typeparam name="T">
            Type contained within the Tensor. Typically a value type such as int, double, float, etc.
            </typeparam>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.#ctor(System.Int32)">
            <summary>
            Initializes a rank-1 Tensor using the specified <paramref name="length"/>.
            </summary>
            <param name="length">Size of the 1-dimensional tensor</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.#ctor(System.ReadOnlySpan{System.Int32},System.Boolean)">
            <summary>
            Initializes a rank-n Tensor using the dimensions specified in <paramref name="dimensions"/>.
            </summary>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the DenseTensor to create.
            </param>
            <param name="reverseStride">
            False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension 
            is most minor (closest together): akin to row-major in a rank-2 tensor.  
            True to indicate that the last dimension is most major (farthest apart) and the first dimension is most 
            minor (closest together): akin to column-major in a rank-2 tensor.
            </param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.#ctor(System.Memory{`0},System.ReadOnlySpan{System.Int32},System.Boolean)">
            <summary>
            Constructs a new DenseTensor of the specified dimensions, wrapping existing backing memory for the contents.
            </summary>
            <param name="memory"></param>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <param name="reverseStride">
            False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension 
            is most minor (closest together): akin to row-major in a rank-2 tensor.  
            True to indicate that the last dimension is most major (farthest apart) and the first dimension is most 
            minor (closest together): akin to column-major in a rank-2 tensor.
            </param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.Buffer">
            <summary>
            Memory storing backing values of this tensor.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.GetValue(System.Int32)">
            <summary>
            Gets the value at the specified index, where index is a linearized version of n-dimension indices 
            using strides. For a scalar, use index = 0
            </summary>
            <param name="index">An integer index computed as a dot-product of indices.</param>
            <returns>The value at the specified position in this Tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.SetValue(System.Int32,`0)">
            <summary>
            Sets the value at the specified index, where index is a linearized version of n-dimension indices 
            using strides. For a scalar, use index = 0
            </summary>
            <param name="index">An integer index computed as a dot-product of indices.</param>
            <param name="value">The new value to set at the specified position in this Tensor.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.CopyTo(`0[],System.Int32)">
            <summary>
            Overrides Tensor.CopyTo(). Copies the content of the Tensor
            to the specified array starting with arrayIndex
            </summary>
            <param name="array">destination array</param>
            <param name="arrayIndex">start index</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.IndexOf(`0)">
            <summary>
            Determines the index of a specific item in the Tensor&lt;T&gt;.
            </summary>
            <param name="item">Object to locate</param>
            <returns>The index of item if found in the tensor; otherwise, -1</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.Clone">
            <summary>
            Creates a shallow copy of this tensor, with new backing storage.
            </summary>
            <returns>A shallow copy of this tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.CloneEmpty``1(System.ReadOnlySpan{System.Int32})">
            <summary>
            Creates a new Tensor of a different type with the specified dimensions and the same layout as this tensor 
            with elements initialized to their default value.
            </summary>
            <typeparam name="TResult">Type contained in the returned Tensor.</typeparam>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <returns>A new tensor with the same layout as this tensor but different type and dimensions.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.Reshape(System.ReadOnlySpan{System.Int32})">
            <summary>
            Reshapes the current tensor to new dimensions, using the same backing storage.
            </summary>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <returns>A new tensor that reinterprets backing Buffer of this tensor with different dimensions.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.TensorElementType">
            <summary>
            Supported Tensor DataType
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo">
            <summary>
            Helps typecasting. Holds Tensor element type traits.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo.ElementType">
            <summary>
            TensorElementType enum
            </summary>
            <value>type enum value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo.TypeSize">
            <summary>
            Size of the stored primitive type in bytes
            </summary>
            <value>size in bytes</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo.IsString">
            <summary>
            Is the type is a string
            </summary>
            <value>true if Tensor element type is a string</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo.#ctor(Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int32)">
            <summary>
            Ctor
            </summary>
            <param name="elementType">TensorElementType value</param>
            <param name="typeSize">size fo the type in bytes</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo">
            <summary>
            Holds TensorElement traits
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo.TensorType">
            <summary>
            Tensor element type
            </summary>
            <value>System.Type</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo.TypeSize">
            <summary>
            Size of the stored primitive type in bytes
            </summary>
            <value>size in bytes</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo.IsString">
            <summary>
            Is the type is a string
            </summary>
            <value>true if Tensor element type is a string</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo.#ctor(System.Type,System.Int32)">
            <summary>
            Ctor
            </summary>
            <param name="type">Tensor element type</param>
            <param name="typeSize">typesize</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.TensorBase">
            <summary>
            This class is a base for all Tensors. It hosts maps with type traits.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorBase.#ctor(System.Type)">
            <summary>
            Constructs TensorBae
            </summary>
            <param name="primitiveType">primitive type the deriving class is using</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorBase.GetTypeInfo(System.Type)">
            <summary>
            Query TensorTypeInfo by one of the supported types
            </summary>
            <param name="type"></param>
            <returns>TensorTypeInfo or null if not supported</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorBase.GetElementTypeInfo(Microsoft.ML.OnnxRuntime.Tensors.TensorElementType)">
            <summary>
            Query TensorElementTypeInfo by enum
            </summary>
            <param name="elementType">type enum</param>
            <returns>instance of TensorElementTypeInfo or null if not found</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorBase.GetTypeInfo">
            <summary>
            Query TensorTypeInfo using this Tensor type
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.Tensor">
            <summary>
            Various methods for creating and manipulating Tensor&lt;T&gt;
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateIdentity``1(System.Int32)">
            <summary>
            Creates an identity tensor of the specified size.  An identity tensor is a two dimensional tensor with 1s in the diagonal.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="size">Width and height of the identity tensor to create.</param>
            <returns>a <paramref name="size"/> by <paramref name="size"/> with 1s along the diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateIdentity``1(System.Int32,System.Boolean)">
            <summary>
            Creates an identity tensor of the specified size and layout (row vs column major).  An identity tensor is a two dimensional tensor with 1s in the diagonal.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="size">Width and height of the identity tensor to create.</param>
            <param name="columMajor">>False to indicate that the first dimension is most minor (closest) and the last dimension is most major (farthest): row-major.  True to indicate that the last dimension is most minor (closest together) and the first dimension is most major (farthest apart): column-major.</param>
            <returns>a <paramref name="size"/> by <paramref name="size"/> with 1s along the diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateIdentity``1(System.Int32,System.Boolean,``0)">
            <summary>
            Creates an identity tensor of the specified size and layout (row vs column major) using the specified one value.  An identity tensor is a two dimensional tensor with 1s in the diagonal.  This may be used in case T is a type that doesn't have a known 1 value.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="size">Width and height of the identity tensor to create.</param>
            <param name="columMajor">>False to indicate that the first dimension is most minor (closest) and the last dimension is most major (farthest): row-major.  True to indicate that the last dimension is most minor (closest together) and the first dimension is most major (farthest apart): column-major.</param>
            <param name="oneValue">Value of <typeparamref name="T"/> that is used along the diagonal.</param>
            <returns>a <paramref name="size"/> by <paramref name="size"/> with 1s along the diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateFromDiagonal``1(Microsoft.ML.OnnxRuntime.Tensors.Tensor{``0})">
            <summary>
            Creates a n+1-rank tensor using the specified n-rank diagonal.  Values not on the diagonal will be filled with zeros.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="diagonal">Tensor representing the diagonal to build the new tensor from.</param>
            <returns>A new tensor of the same layout and order as <paramref name="diagonal"/> of one higher rank, with the values of <paramref name="diagonal"/> along the diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateFromDiagonal``1(Microsoft.ML.OnnxRuntime.Tensors.Tensor{``0},System.Int32)">
            <summary>
            Creates a n+1-dimension tensor using the specified n-dimension diagonal at the specified offset 
            from the center.  Values not on the diagonal will be filled with zeros.
            </summary>
            <typeparam name="T">
            type contained within the Tensor. Typically a value type such as int, double, float, etc.</typeparam>
            <param name="diagonal">Tensor representing the diagonal to build the new tensor from.</param>
            <param name="offset">Offset of diagonal to set in returned tensor.  0 for the main diagonal, 
            less than zero for diagonals below, greater than zero from diagonals above.</param>
            <returns>A new tensor of the same layout and order as <paramref name="diagonal"/> of one higher rank, 
            with the values of <paramref name="diagonal"/> along the specified diagonal and zeros elsewhere.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1">
            <summary>
            Represents a multi-dimensional collection of objects of type T that can be accessed by indices.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.#ctor(System.Int32)">
            <summary>
            Initialize a 1-dimensional tensor of the specified length
            </summary>
            <param name="length">Size of the 1-dimensional tensor</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.#ctor(System.ReadOnlySpan{System.Int32},System.Boolean)">
            <summary>
            Initialize an n-dimensional tensor with the specified dimensions and layout.  
            ReverseStride=true gives a stride of 1-element width to the first dimension (0).  
            ReverseStride=false gives a stride of 1-element width to the last dimension (n-1).
            </summary>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the Tensor to create.</param>
            <param name="reverseStride">
            False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension 
            is most minor (closest together): akin to row-major in a rank-2 tensor.  
            True to indicate that the last dimension is most major (farthest apart) and the first dimension is most 
            minor (closest together): akin to column-major in a rank-2 tensor.</param>
            <remarks>If you pass `null` for dimensions it will implicitly convert to an empty ReadOnlySpan, which is 
            equivalent to the dimensions for a scalar value.</remarks>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.#ctor(System.Array,System.Boolean)">
            <summary>
            Initializes tensor with same dimensions as array, content of array is ignored.  
            ReverseStride=true gives a stride of 1-element width to the first dimension (0).  
            ReverseStride=false gives a stride of 1-element width to the last dimension (n-1).
            </summary>
            <param name="fromArray">Array from which to derive dimensions.</param>
            <param name="reverseStride">
            False (default) to indicate that the first dimension is most major (farthest apart) and the 
            last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  
            True to indicate that the last dimension is most major (farthest apart) and the first dimension 
            is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Length">
            <summary>
            Total length of the Tensor.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Rank">
            <summary>
            Rank of the tensor: number of dimensions.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.IsReversedStride">
            <summary>
            True if strides are reversed (AKA Column-major)
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Dimensions">
            <summary>
            Returns a readonly view of the dimensions of this tensor.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Strides">
            <summary>
            Returns a readonly view of the strides of this tensor.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Fill(`0)">
            <summary>
            Sets all elements in Tensor to <paramref name="value"/>.
            </summary>
            <param name="value">Value to fill</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Clone">
            <summary>
            Creates a shallow copy of this tensor, with new backing storage.
            </summary>
            <returns>A shallow copy of this tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CloneEmpty">
            <summary>
            Creates a new Tensor with the same layout and dimensions as this tensor with elements initialized to their default value.
            </summary>
            <returns>A new Tensor with the same layout and dimensions as this tensor with elements initialized to their default value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CloneEmpty(System.ReadOnlySpan{System.Int32})">
            <summary>
            Creates a new Tensor with the specified dimensions and the same layout as this tensor with elements initialized to their default value.
            </summary>
            <param name="dimensions">An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <returns>A new Tensor with the same layout as this tensor and specified <paramref name="dimensions"/> with elements initialized to their default value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CloneEmpty``1">
            <summary>
            Creates a new Tensor of a different type with the same layout and size as this tensor with elements initialized to their default value.
            </summary>
            <typeparam name="TResult">Type contained within the new Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <returns>A new Tensor with the same layout and dimensions as this tensor with elements of <typeparamref name="TResult"/> type initialized to their default value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CloneEmpty``1(System.ReadOnlySpan{System.Int32})">
            <summary>
            Creates a new Tensor of a different type with the specified dimensions and the same layout as this tensor with elements initialized to their default value.
            </summary>
            <typeparam name="TResult">Type contained within the new Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="dimensions">An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <returns>A new Tensor with the same layout as this tensor of specified <paramref name="dimensions"/> with elements of <typeparamref name="TResult"/> type initialized to their default value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetDiagonal">
            <summary>
            Gets the n-1 dimension diagonal from the n dimension tensor.
            </summary>
            <returns>An n-1 dimension tensor with the values from the main diagonal of this tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetDiagonal(System.Int32)">
            <summary>
            Gets the n-1 dimension diagonal from the n dimension tensor at the specified offset from center.
            </summary>
            <param name="offset">Offset of diagonal to set in returned tensor.  0 for the main diagonal, less than zero for diagonals below, greater than zero from diagonals above.</param>
            <returns>An n-1 dimension tensor with the values from the specified diagonal of this tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetTriangle">
            <summary>
            Gets a tensor representing the elements below and including the diagonal, with the rest of the elements zero-ed.
            </summary>
            <returns>A tensor with the values from this tensor at and below the main diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetTriangle(System.Int32)">
            <summary>
            Gets a tensor representing the elements below and including the specified diagonal, with the rest of the elements zero-ed.
            </summary>
            <param name="offset">Offset of diagonal to set in returned tensor.  0 for the main diagonal, less than zero for diagonals below, greater than zero from diagonals above.</param>
            <returns>A tensor with the values from this tensor at and below the specified diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetUpperTriangle">
            <summary>
            Gets a tensor representing the elements above and including the diagonal, with the rest of the elements zero-ed.
            </summary>
            <returns>A tensor with the values from this tensor at and above the main diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetUpperTriangle(System.Int32)">
            <summary>
            Gets a tensor representing the elements above and including the specified diagonal, with the rest of the elements zero-ed.
            </summary>
            <param name="offset">Offset of diagonal to set in returned tensor.  0 for the main diagonal, less than zero for diagonals below, greater than zero from diagonals above.</param>
            <returns>A tensor with the values from this tensor at and above the specified diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetTriangle(System.Int32,System.Boolean)">
            <summary>
            Implementation method for GetTriangle, GetLowerTriangle, GetUpperTriangle
            </summary>
            <param name="offset">Offset of diagonal to set in returned tensor.</param>
            <param name="upper">true for upper triangular and false otherwise</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Reshape(System.ReadOnlySpan{System.Int32})">
            <summary>
            Reshapes the current tensor to new dimensions, using the same backing storage if possible.
            </summary>
            <param name="dimensions">An span of integers that represent the size of each dimension of the Tensor to create.</param>
            <returns>A new tensor that reinterprets this tensor with different dimensions.</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Item(System.Int32[])">
            <summary>
            Obtains the value at the specified indices
            </summary>
            <param name="indices">A one-dimensional array of integers that represent the indices specifying the position of the element to get.</param>
            <returns>The value at the specified position in this Tensor.</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Item(System.ReadOnlySpan{System.Int32})">
            <summary>
            Obtains the value at the specified indices
            </summary>
            <param name="indices">A span integers that represent the indices specifying the position of the element to get.</param>
            <returns>The value at the specified position in this Tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetValue(System.Int32)">
            <summary>
            Gets the value at the specied index, where index is a linearized version of n-dimension indices using strides.
            </summary>
            <param name="index">An integer index computed as a dot-product of indices.</param>
            <returns>The value at the specified position in this Tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.SetValue(System.Int32,`0)">
            <summary>
            Sets the value at the specied index, where index is a linearized version of n-dimension indices using strides.
            </summary>
            <param name="index">An integer index computed as a dot-product of indices.</param>
            <param name="value">The new value to set at the specified position in this Tensor.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Compare(Microsoft.ML.OnnxRuntime.Tensors.Tensor{`0},Microsoft.ML.OnnxRuntime.Tensors.Tensor{`0})">
            <summary>
            Performs a value comparison of the content and shape of two tensors.  Two tensors are equal if they have the same shape and same value at every set of indices.  If not equal a tensor is greater or less than another tensor based on the first non-equal element when enumerating in linear order.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Equals(Microsoft.ML.OnnxRuntime.Tensors.Tensor{`0},Microsoft.ML.OnnxRuntime.Tensors.Tensor{`0})">
            <summary>
            Performs a value equality comparison of the content of two tensors. Two tensors are equal if they have the same shape and same value at every set of indices.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns></returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.IsFixedSize">
            <summary>
            Always fixed size Tensor
            </summary>
            <value>always true</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.IsReadOnly">
            <summary>
            Tensor is not readonly
            </summary>
            <value>always false</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Contains(`0)">
            <summary>
            Determines whether an element is in the Tensor&lt;T&gt;.
            </summary>
            <param name="item">
            The object to locate in the Tensor&lt;T&gt;. The value can be null for reference types.
            </param>
            <returns>
            true if item is found in the Tensor&lt;T&gt;; otherwise, false.
            </returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CopyTo(`0[],System.Int32)">
            <summary>
            Copies the elements of the Tensor&lt;T&gt; to an Array, starting at a particular Array index.
            </summary>
            <param name="array">
            The one-dimensional Array that is the destination of the elements copied from Tensor&lt;T&gt;. The Array must have zero-based indexing.
            </param>
            <param name="arrayIndex">
            The zero-based index in array at which copying begins.
            </param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.IndexOf(`0)">
            <summary>
            Determines the index of a specific item in the Tensor&lt;T&gt;.
            </summary>
            <param name="item">The object to locate in the Tensor&lt;T&gt;.</param>
            <returns>The index of item if found in the tensor; otherwise, -1.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.ToDenseTensor">
            <summary>
            Creates a copy of this tensor as a DenseTensor&lt;T&gt;.  If this tensor is already a DenseTensor&lt;T&gt; calling this method is equivalent to calling Clone().
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetArrayString(System.Boolean)">
            <summary>
            Get a string representation of Tensor
            </summary>
            <param name="includeWhitespace"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.CheckpointState">
            <summary>
             Holds the state of the training session.
            This class holds the entire training session state that includes model parameters, their gradients,
            optimizer parameters, and user properties. The TrainingSession leverages the CheckpointState
            by accessing and updating the contained training state.
            <note type="note">
            Note that the training session created with a checkpoint state uses this state to store the entire
            training state (including model parameters, its gradients, the optimizer states and the properties).
            The TrainingSession does not hold a copy of the CheckpointState and as a result, it is required
            that the checkpoint state outlives the lifetime of the training session.
            </note>
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.CheckpointState.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.LoadCheckpoint(System.String)">
             <summary>
             Load a checkpoint state from a directory on disk into checkpoint_state.
            
             This function will parse a checkpoint directory, pull relevant files and load the training
             state into the checkpoint_state. This checkpoint state can then be used to create the
             training session by instantiating the TrainingSession. By doing so, the training
             session will begin or resume training from the given checkpoint state.
             </summary>
             <param name="checkpointPath"> Absolute path to the checkpoint directory.</param>
             <returns>CheckpointState object which holds the state of the training session parameters.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.SaveCheckpoint(Microsoft.ML.OnnxRuntime.CheckpointState,System.String,System.Boolean)">
             <summary>
             Save the given state to a checkpoint directory on disk.
            
             This function serializes the provided checkpoint state to a directory on disk.
             This checkpoint can later be loaded by invoking CheckpointState.LoadCheckpoint to begin or resume
             training from this snapshot of the state.
             </summary>
             <param name="state"> The checkpoint state to save.</param>
             <param name="checkpointPath"> Absolute path to the checkpoint directory.</param>
             <param name="includeOptimizerState"> Flag to indicate whether to save the optimizer state or not.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.AddProperty(System.String,System.Int64)">
             <summary>
             Adds or updates the given int property to/in the checkpoint state.
            
             Runtime properties such as epoch, training step, best score, and others can be added to the checkpoint
             state by the user by calling this function with the corresponding property name and value.
             The given property name must be unique to be able to successfully add the property.
             </summary>
             <param name="propertyName">Name of the property being added or updated.</param>
             <param name="propertyValue">Property value associated with the given name.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.AddProperty(System.String,System.Single)">
             <summary>
             Adds or updates the given float property to/in the checkpoint state.
            
             Runtime properties such as epoch, training step, best score, and others can be added to the checkpoint
             state by the user by calling this function with the corresponding property name and value.
             The given property name must be unique to be able to successfully add the property.
             </summary>
             <param name="propertyName">Name of the property being added or updated.</param>
             <param name="propertyValue">Property value associated with the given name.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.AddProperty(System.String,System.String)">
             <summary>
             Adds or updates the given string property to/in the checkpoint state.
            
             Runtime properties such as epoch, training step, best score, and others can be added to the checkpoint
             state by the user by calling this function with the corresponding property name and value.
             The given property name must be unique to be able to successfully add the property.
             </summary>
             <param name="propertyName">Name of the property being added or updated.</param>
             <param name="propertyValue">Property value associated with the given name.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.GetProperty(System.String)">
             <summary>
             Gets the property value associated with the given name from the checkpoint state.
            
             Gets the property value from an existing entry in the checkpoint state. The property must
             exist in the checkpoint state to be able to retrieve it successfully.
             </summary>
             <param name="propertyName">Name of the property being retrieved.</param>
             <returns>Property value associated with the given property name.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.UpdateParameter(System.String,Microsoft.ML.OnnxRuntime.OrtValue)">
             <summary>
             Updates the data associated with the model parameter in the checkpoint state for the given parameter name.
            
             This function updates a model parameter in the checkpoint state with the given parameter data.
             The training session must be already created with the checkpoint state that contains the parameter
             being updated. The given parameter is copied over to the registered device for the training session.
             The parameter must exist in the checkpoint state to be able to update it successfully.
             </summary>
             <param name="parameterName">Name of the parameter being updated.</param>
             <param name="parameter">The parameter data that should replace the existing parameter data.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.GetParameter(System.String)">
             <summary>
             Gets the data associated with the model parameter from the checkpoint state for the given parameter name.
            
             This function retrieves the model parameter data from the checkpoint state for the given parameter name.
             The parameter is copied over to the provided OrtValue. The training session must be already created
             with the checkpoint state that contains the parameter being retrieved.
             The parameter must exist in the checkpoint state to be able to retrieve it successfully.
             </summary>
             <param name="parameterName">Name of the parameter being updated.</param>
             <returns>The parameter data that is retrieved from the checkpoint state.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of CheckpointState
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeTrainingMethods.DOrtLoadCheckpoint">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="checkpointPath">checkpoint string path</param>
            <param name="checkpointState">(Output) Loaded OrtCheckpointState instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeTrainingMethods.DOrtSaveCheckpoint">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="checkpointState">OrtCheckpointState instance to save</param>
            <param name="checkpointPath">Checkpoint string path</param>
            <param name="includeOptimizerState">Flag indicating whether to save the optimizer state.</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeTrainingMethods.DOrtCreateTrainingSession">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="environment">Native OrtEnv instance</param>
            <param name="sessionOptions">Native SessionOptions instance</param>
            <param name="checkpointState">Loaded OrtCheckpointState instance</param>
            <param name="trainModelPath">model string path</param>
            <param name="evalModelPath">model string path</param>
            <param name="optimizerModelPath">model string path</param>
            <param name="session">(Output) Created native OrtTrainingSession instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.TrainingUtils">
            <summary>
            This class defines utility methods for training.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingUtils.SetSeed(System.Int64)">
            <summary>
            Use this function to generate reproducible results. It should be noted that completely
            reproducible results are not guaranteed.
            </summary>
            <param name="seed">Manual seed to use for random number generation.</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.TrainingSession">
             <summary>
             Trainer class that provides training, evaluation and optimizer methods for training an ONNX model.
            
             The training session requires four training artifacts
             - The training onnx model
             - The evaluation onnx model (optional)
             - The optimizer onnx model
             - The checkpoint directory
            
             These artifacts can be generated using the `onnxruntime-training` python [utility](https://github.com/microsoft/onnxruntime/blob/main/orttraining/orttraining/python/training/onnxblock/README.md).
            
             This is an IDisposable class and it must be disposed of
             using either an explicit call to Dispose() method or
             a pattern of using() block. If this is a member of another
             class that class must also become IDisposable and it must
             dispose of TrainingSession in its Dispose() method.
             </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.TrainingSession._nativeHandle">
            <summary>
            A pointer to an underlying native instance of OrtTrainingSession
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.#ctor(Microsoft.ML.OnnxRuntime.CheckpointState,System.String,System.String,System.String)">
             <summary>
             Create a training session that can be used to begin or resume training.
            
             This constructor instantiates the training session based on the env and session options provided that can
             begin or resume training from a given checkpoint state for the given onnx models.
             The checkpoint state represents the parameters of the training session which will be moved
             to the device specified by the user through the session options (if necessary).
             </summary>
             <param name="state">Training states that the training session uses as a starting point for training.</param>
             <param name="trainModelPath">Model to be used to perform training.</param>
             <param name="evalModelPath">Model to be used to perform evaluation.</param>
             <param name="optimizerModelPath">Model to be used to perform weight update.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.#ctor(Microsoft.ML.OnnxRuntime.CheckpointState,System.String,System.String)">
             <summary>
             Create a training session that can be used to begin or resume training.
            
             This constructor instantiates the training session based on the env and session options provided that can
             begin or resume training from a given checkpoint state for the given onnx models.
             The checkpoint state represents the parameters of the training session which will be moved
             to the device specified by the user through the session options (if necessary).
             </summary>
             <param name="state">Training states that the training session uses as a starting point for training.</param>
             <param name="trainModelPath">Model to be used to perform training.</param>
             <param name="optimizerModelPath">Model to be used to perform weight update.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.#ctor(Microsoft.ML.OnnxRuntime.SessionOptions,Microsoft.ML.OnnxRuntime.CheckpointState,System.String,System.String,System.String)">
             <summary>
             Create a training session that can be used to begin or resume training.
            
             This constructor instantiates the training session based on the env and session options provided that can
             begin or resume training from a given checkpoint state for the given onnx models.
             The checkpoint state represents the parameters of the training session which will be moved
             to the device specified by the user through the session options (if necessary).
             </summary>
             <param name="options">SessionOptions that the user can customize for this training session.</param>
             <param name="state">Training states that the training session uses as a starting point for training.</param>
             <param name="trainModelPath">Model to be used to perform training.</param>
             <param name="evalModelPath">Model to be used to perform evaluation.</param>
             <param name="optimizerModelPath">Model to be used to perform weight update.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.TrainStep(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
             <summary>
             Computes the outputs of the training model and the gradients of the trainable parameters for the given inputs
            
             This function performs a training step that computes the outputs of the training model and the gradients
             of the trainable parameters for the given inputs. The train step is performed based on the training model
             that was provided to the training session.
             The TrainStep method is equivalent of running forward propagation and backward propagation in a single
             step.
             The gradients computed are stored inside the training session state so they can be later consumed
             by the OptimizerStep function.
             The gradients can be lazily reset by invoking the LazyResetGrad function.
             </summary>
             <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values to the training model.</param>
             <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values of the training model.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.TrainStep(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
             <summary>
             Computes the outputs of the training model and the gradients of the trainable parameters for the given inputs
            
             This function performs a training step that computes the outputs of the training model and the gradients
             of the trainable parameters for the given inputs. The train step is performed based on the training model
             that was provided to the training session.
             The TrainStep method is equivalent of running forward propagation and backward propagation in a single
             step.
             The gradients computed are stored inside the training session state so they can be later consumed
             by the OptimizerStep function.
             The gradients can be lazily reset by invoking the LazyResetGrad function.
             </summary>
             <param name="options">Specify <see cref="T:Microsoft.ML.OnnxRuntime.RunOptions"/> for step.</param>
             <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values to the training model.</param>
             <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values of the training model.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.TrainStep(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
             <summary>
             Computes the outputs of the training model and the gradients of the trainable parameters for the given inputs
            
             This function performs a training step that computes the outputs of the training model and the gradients
             of the trainable parameters for the given inputs. The train step is performed based on the training model
             that was provided to the training session.
             The TrainStep method is equivalent of running forward propagation and backward propagation in a single
             step.
             The gradients computed are stored inside the training session state so they can be later consumed
             by the OptimizerStep function.
             The gradients can be lazily reset by invoking the LazyResetGrad function.
             </summary>
             <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values to the training model.</param>
             <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.TrainStep(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
             <summary>
             Computes the outputs of the training model and the gradients of the trainable parameters for the given inputs
            
             This function performs a training step that computes the outputs of the training model and the gradients
             of the trainable parameters for the given inputs. The train step is performed based on the training model
             that was provided to the training session.
             The TrainStep method is equivalent of running forward propagation and backward propagation in a single
             step.
             The gradients computed are stored inside the training session state so they can be later consumed
             by the OptimizerStep function.
             The gradients can be lazily reset by invoking the LazyResetGrad function.
             </summary>
             <param name="options">Specify <see cref="T:Microsoft.ML.OnnxRuntime.RunOptions"/> for step.</param>
             <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values to the training model.</param>
             <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.ConvertNativeHandlesToOrtValues(System.IntPtr[])">
            <summary>
            Convert native OrtValue handles to OrtValue instances
            in an exceptions safe manner.
            </summary>
            <param name="nativeHandles"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.LazyResetGrad">
             <summary>
             Reset the gradients of all trainable parameters to zero lazily.
            
             This function sets the internal state of the training session such that the gradients of the trainable
             parameters in the OrtCheckpointState will be scheduled to be reset just before the new gradients are
             computed on the next invocation of the next TrainStep.
             </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.EvalStep(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Computes the outputs for the eval model for the given inputs
            This function performs an eval step that computes the outputs of the eval model for the given inputs.
            The eval step is performed based on the eval model that was provided to the training session.
            </summary>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values to the eval model.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values of the eval model.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.EvalStep(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Computes the outputs for the eval model for the given inputs
            This function performs an eval step that computes the outputs of the eval model for the given inputs.
            The eval step is performed based on the eval model that was provided to the training session.
            </summary>
            <param name="options">Specify <see cref="T:Microsoft.ML.OnnxRuntime.RunOptions"/> for step.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values to the eval model.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values of the eval model.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.SetLearningRate(System.Single)">
             <summary>
             Sets the learning rate for this training session.
            
             This function allows users to set the learning rate for the training session. The current
             learning rate is maintained by the training session and can be overwritten by invoking
             this function with the desired learning rate. This function should not be used when a valid
             learning rate scheduler is registered. It should be used either to set the learning rate
             derived from a custom learning rate scheduler or to set a constant learning rate to be used
             throughout the training session.
             <note type="note">
             Please note that this function does not set the initial learning rate that may be needed
             by the predefined learning rate schedulers. To set the initial learning rate for learning
             rate schedulers, please look at the function RegisterLinearLRScheduler.
             </note>
             </summary>
             <param name="learningRate">Desired learning rate to be set.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.GetLearningRate">
             <summary>
             Gets the current learning rate for this training session.
            
             This function allows users to get the learning rate for the training session. The current
             learning rate is maintained by the training session, and users can query it for the purpose
             of implementing their own learning rate schedulers.
             </summary>
             <returns>float representing the current learning rate.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.RegisterLinearLRScheduler(System.Int64,System.Int64,System.Single)">
             <summary>
             Registers a linear learning rate scheduler for the training session.
            
             Register a linear learning rate scheduler that decays the learning rate by linearly updated
             multiplicative factor from the initial learning rate set on the training session to 0. The decay
             is performed after the initial warm up phase where the learning rate is linearly incremented
             from 0 to the initial learning rate provided.
             </summary>
             <param name="warmupStepCount"> Number of warmup steps</param>
             <param name="totalStepCount"> Number of total steps</param>
             <param name="initialLearningRate"> Initial learning rate</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.SchedulerStep">
             <summary>
             Update the learning rate based on the registered learning rate scheduler.
            
             Takes a scheduler step that updates the learning rate that is being used by the training session.
             This function should typically be called before invoking the optimizer step for each round,
             or as determined necessary to update the learning rate being used by the training session.
             <note type="note">
             Please note that a valid predefined learning rate scheduler must be first registered to invoke this function.
             </note>
             </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.OptimizerStep">
             <summary>
             Performs the weight updates for the trainable parameters using the optimizer model.
            
             This function performs the weight update step that updates the trainable parameters such that they
             take a step in the direction of their gradients (gradient descent). The optimizer step is performed
             based on the optimizer model that was provided to the training session.
             The updated parameters are stored inside the training state so that they can be used by the next
             TrainStep function call.
             </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.OptimizerStep(Microsoft.ML.OnnxRuntime.RunOptions)">
             <summary>
             Performs the weight updates for the trainable parameters using the optimizer model.
            
             This function performs the weight update step that updates the trainable parameters such that they
             take a step in the direction of their gradients (gradient descent). The optimizer step is performed
             based on the optimizer model that was provided to the training session.
             The updated parameters are stored inside the training state so that they can be used by the next
             TrainStep function call.
             </summary>
             <param name="options">Specify <see cref="T:Microsoft.ML.OnnxRuntime.RunOptions"/> for step.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.ExportModelForInferencing(System.String,System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            Export a model that can be used for inferencing.
            If the training session was provided with an eval model, the training session can generate
            an inference model if it knows the inference graph outputs. The input inference graph outputs
            are used to prune the eval model so that the inference model's outputs align with the provided outputs.
            The exported model is saved at the path provided and can be used for inferencing with InferenceSession.
            Note that the function re-loads the eval model from the path provided to TrainingSession
            and expects that this path still be valid.
            </summary>
            <param name="inferenceModelPath">Path where the inference model should be serialized to.</param>
            <param name="graphOutputNames">Names of the outputs that are needed in the inference model.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.ToBuffer(System.Boolean)">
            <summary>
            Returns a contiguous buffer that holds a copy of all training state parameters
            </summary>
            <param name="onlyTrainable">Whether to only copy trainable parameters or to copy all parameters.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.FromBuffer(Microsoft.ML.OnnxRuntime.OrtValue,System.Boolean)">
            <summary>
            Loads the training session model parameters from a contiguous buffer
            </summary>
            <param name="ortValue">Contiguous buffer to load the parameters from.</param>
            <param name="onlyTrainable">Whether to only load trainable parameters or to load all parameters.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.OutputNames(System.Boolean)">
            <summary>
            Retrieves the names of the user outputs for the training and eval models.
            </summary>
            <param name="training">Whether the training model output names are requested or eval model output names.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.InputNames(System.Boolean)">
            <summary>
            Retrieves the names of the user inputs for the training and eval models.
            </summary>
            <param name="training">Whether the training model input names are requested or eval model input names.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TrainingSession.Handle">
            <summary>
            Other classes access
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.Finalize">
            <summary>
            Finalizer.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked from Dispose() method</param>
        </member>
    </members>
</doc>
